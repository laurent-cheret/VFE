{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TKEfNGBAJfk"
      },
      "source": [
        "# Encoding High-Level Features: An Approach to Robust Transfer Learning.\n",
        "## This code shows variational autoencoders being used to reduce dimensionality of feaure maps and provide the encoded representation to an image classifier. This work is an attempt to improve the robustness of the classic image classification architecture by performing a simple change to the overall system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXuwBRbYAGmQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGJ-3HbVJQYU"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(image, label):\n",
        "\n",
        "  image2 = tf.keras.applications.densenet.preprocess_input(image)\n",
        "  return image2, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxLo_RxBJeFj"
      },
      "outputs": [],
      "source": [
        "#### CREATE BASE MODEL FOR FEATURE EXTRACTION #####\n",
        "\n",
        "def base_model():\n",
        "\n",
        "  base_model = tf.keras.applications.DenseNet121(include_top= False, weights = 'imagenet', input_shape = (224,224,3))\n",
        "  print(len(base_model.layers))\n",
        "  for layer in base_model.layers[:200]:\n",
        "    layer.trainable = False\n",
        "  for layer in base_model.layers[200:]:\n",
        "    layer.trainable = True\n",
        "  base_model.summary()\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Lambda(lambda img: tf.image.resize(img, (224,224))))\n",
        "  model.add(base_model)\n",
        "  model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8FlfNpJJuu0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20bdf905-2ff9-4e4d-afd3-b04cf4a6c077"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 0s 0us/step\n",
            "29097984/29084464 [==============================] - 0s 0us/step\n",
            "427\n",
            "Model: \"densenet121\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " zero_padding2d (ZeroPadding2D)  (None, 230, 230, 3)  0          ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv1/conv (Conv2D)            (None, 112, 112, 64  9408        ['zero_padding2d[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv1/bn (BatchNormalization)  (None, 112, 112, 64  256         ['conv1/conv[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv1/relu (Activation)        (None, 112, 112, 64  0           ['conv1/bn[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " zero_padding2d_1 (ZeroPadding2  (None, 114, 114, 64  0          ['conv1/relu[0][0]']             \n",
            " D)                             )                                                                 \n",
            "                                                                                                  \n",
            " pool1 (MaxPooling2D)           (None, 56, 56, 64)   0           ['zero_padding2d_1[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block1_0_bn (BatchNormal  (None, 56, 56, 64)  256         ['pool1[0][0]']                  \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_0_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2D)   (None, 56, 56, 128)  8192        ['conv2_block1_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv2_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activatio  (None, 56, 56, 128)  0          ['conv2_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2D)   (None, 56, 56, 32)   36864       ['conv2_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_concat (Concatena  (None, 56, 56, 96)  0           ['pool1[0][0]',                  \n",
            " te)                                                              'conv2_block1_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_0_bn (BatchNormal  (None, 56, 56, 96)  384         ['conv2_block1_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_0_relu (Activatio  (None, 56, 56, 96)  0           ['conv2_block2_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2D)   (None, 56, 56, 128)  12288       ['conv2_block2_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv2_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activatio  (None, 56, 56, 128)  0          ['conv2_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2D)   (None, 56, 56, 32)   36864       ['conv2_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_concat (Concatena  (None, 56, 56, 128)  0          ['conv2_block1_concat[0][0]',    \n",
            " te)                                                              'conv2_block2_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_0_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv2_block2_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_0_relu (Activatio  (None, 56, 56, 128)  0          ['conv2_block3_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2D)   (None, 56, 56, 128)  16384       ['conv2_block3_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv2_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activatio  (None, 56, 56, 128)  0          ['conv2_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2D)   (None, 56, 56, 32)   36864       ['conv2_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_concat (Concatena  (None, 56, 56, 160)  0          ['conv2_block2_concat[0][0]',    \n",
            " te)                                                              'conv2_block3_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block4_0_bn (BatchNormal  (None, 56, 56, 160)  640        ['conv2_block3_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block4_0_relu (Activatio  (None, 56, 56, 160)  0          ['conv2_block4_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block4_1_conv (Conv2D)   (None, 56, 56, 128)  20480       ['conv2_block4_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block4_1_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv2_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block4_1_relu (Activatio  (None, 56, 56, 128)  0          ['conv2_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block4_2_conv (Conv2D)   (None, 56, 56, 32)   36864       ['conv2_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block4_concat (Concatena  (None, 56, 56, 192)  0          ['conv2_block3_concat[0][0]',    \n",
            " te)                                                              'conv2_block4_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block5_0_bn (BatchNormal  (None, 56, 56, 192)  768        ['conv2_block4_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block5_0_relu (Activatio  (None, 56, 56, 192)  0          ['conv2_block5_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block5_1_conv (Conv2D)   (None, 56, 56, 128)  24576       ['conv2_block5_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block5_1_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv2_block5_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block5_1_relu (Activatio  (None, 56, 56, 128)  0          ['conv2_block5_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block5_2_conv (Conv2D)   (None, 56, 56, 32)   36864       ['conv2_block5_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block5_concat (Concatena  (None, 56, 56, 224)  0          ['conv2_block4_concat[0][0]',    \n",
            " te)                                                              'conv2_block5_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block6_0_bn (BatchNormal  (None, 56, 56, 224)  896        ['conv2_block5_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block6_0_relu (Activatio  (None, 56, 56, 224)  0          ['conv2_block6_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block6_1_conv (Conv2D)   (None, 56, 56, 128)  28672       ['conv2_block6_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block6_1_bn (BatchNormal  (None, 56, 56, 128)  512        ['conv2_block6_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block6_1_relu (Activatio  (None, 56, 56, 128)  0          ['conv2_block6_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block6_2_conv (Conv2D)   (None, 56, 56, 32)   36864       ['conv2_block6_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block6_concat (Concatena  (None, 56, 56, 256)  0          ['conv2_block5_concat[0][0]',    \n",
            " te)                                                              'conv2_block6_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " pool2_bn (BatchNormalization)  (None, 56, 56, 256)  1024        ['conv2_block6_concat[0][0]']    \n",
            "                                                                                                  \n",
            " pool2_relu (Activation)        (None, 56, 56, 256)  0           ['pool2_bn[0][0]']               \n",
            "                                                                                                  \n",
            " pool2_conv (Conv2D)            (None, 56, 56, 128)  32768       ['pool2_relu[0][0]']             \n",
            "                                                                                                  \n",
            " pool2_pool (AveragePooling2D)  (None, 28, 28, 128)  0           ['pool2_conv[0][0]']             \n",
            "                                                                                                  \n",
            " conv3_block1_0_bn (BatchNormal  (None, 28, 28, 128)  512        ['pool2_pool[0][0]']             \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_0_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_1_conv (Conv2D)   (None, 28, 28, 128)  16384       ['conv3_block1_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_2_conv (Conv2D)   (None, 28, 28, 32)   36864       ['conv3_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_concat (Concatena  (None, 28, 28, 160)  0          ['pool2_pool[0][0]',             \n",
            " te)                                                              'conv3_block1_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_0_bn (BatchNormal  (None, 28, 28, 160)  640        ['conv3_block1_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_0_relu (Activatio  (None, 28, 28, 160)  0          ['conv3_block2_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_1_conv (Conv2D)   (None, 28, 28, 128)  20480       ['conv3_block2_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_2_conv (Conv2D)   (None, 28, 28, 32)   36864       ['conv3_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_concat (Concatena  (None, 28, 28, 192)  0          ['conv3_block1_concat[0][0]',    \n",
            " te)                                                              'conv3_block2_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_0_bn (BatchNormal  (None, 28, 28, 192)  768        ['conv3_block2_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_0_relu (Activatio  (None, 28, 28, 192)  0          ['conv3_block3_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_1_conv (Conv2D)   (None, 28, 28, 128)  24576       ['conv3_block3_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_2_conv (Conv2D)   (None, 28, 28, 32)   36864       ['conv3_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_concat (Concatena  (None, 28, 28, 224)  0          ['conv3_block2_concat[0][0]',    \n",
            " te)                                                              'conv3_block3_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_0_bn (BatchNormal  (None, 28, 28, 224)  896        ['conv3_block3_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_0_relu (Activatio  (None, 28, 28, 224)  0          ['conv3_block4_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_1_conv (Conv2D)   (None, 28, 28, 128)  28672       ['conv3_block4_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_2_conv (Conv2D)   (None, 28, 28, 32)   36864       ['conv3_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_concat (Concatena  (None, 28, 28, 256)  0          ['conv3_block3_concat[0][0]',    \n",
            " te)                                                              'conv3_block4_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block5_0_bn (BatchNormal  (None, 28, 28, 256)  1024       ['conv3_block4_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block5_0_relu (Activatio  (None, 28, 28, 256)  0          ['conv3_block5_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block5_1_conv (Conv2D)   (None, 28, 28, 128)  32768       ['conv3_block5_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block5_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block5_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block5_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block5_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block5_2_conv (Conv2D)   (None, 28, 28, 32)   36864       ['conv3_block5_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block5_concat (Concatena  (None, 28, 28, 288)  0          ['conv3_block4_concat[0][0]',    \n",
            " te)                                                              'conv3_block5_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block6_0_bn (BatchNormal  (None, 28, 28, 288)  1152       ['conv3_block5_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block6_0_relu (Activatio  (None, 28, 28, 288)  0          ['conv3_block6_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block6_1_conv (Conv2D)   (None, 28, 28, 128)  36864       ['conv3_block6_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block6_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block6_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block6_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block6_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block6_2_conv (Conv2D)   (None, 28, 28, 32)   36864       ['conv3_block6_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block6_concat (Concatena  (None, 28, 28, 320)  0          ['conv3_block5_concat[0][0]',    \n",
            " te)                                                              'conv3_block6_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block7_0_bn (BatchNormal  (None, 28, 28, 320)  1280       ['conv3_block6_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block7_0_relu (Activatio  (None, 28, 28, 320)  0          ['conv3_block7_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block7_1_conv (Conv2D)   (None, 28, 28, 128)  40960       ['conv3_block7_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block7_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block7_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block7_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block7_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block7_2_conv (Conv2D)   (None, 28, 28, 32)   36864       ['conv3_block7_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block7_concat (Concatena  (None, 28, 28, 352)  0          ['conv3_block6_concat[0][0]',    \n",
            " te)                                                              'conv3_block7_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block8_0_bn (BatchNormal  (None, 28, 28, 352)  1408       ['conv3_block7_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block8_0_relu (Activatio  (None, 28, 28, 352)  0          ['conv3_block8_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block8_1_conv (Conv2D)   (None, 28, 28, 128)  45056       ['conv3_block8_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block8_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block8_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block8_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block8_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block8_2_conv (Conv2D)   (None, 28, 28, 32)   36864       ['conv3_block8_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block8_concat (Concatena  (None, 28, 28, 384)  0          ['conv3_block7_concat[0][0]',    \n",
            " te)                                                              'conv3_block8_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block9_0_bn (BatchNormal  (None, 28, 28, 384)  1536       ['conv3_block8_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block9_0_relu (Activatio  (None, 28, 28, 384)  0          ['conv3_block9_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block9_1_conv (Conv2D)   (None, 28, 28, 128)  49152       ['conv3_block9_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block9_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block9_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block9_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block9_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block9_2_conv (Conv2D)   (None, 28, 28, 32)   36864       ['conv3_block9_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block9_concat (Concatena  (None, 28, 28, 416)  0          ['conv3_block8_concat[0][0]',    \n",
            " te)                                                              'conv3_block9_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block10_0_bn (BatchNorma  (None, 28, 28, 416)  1664       ['conv3_block9_concat[0][0]']    \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv3_block10_0_relu (Activati  (None, 28, 28, 416)  0          ['conv3_block10_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block10_1_conv (Conv2D)  (None, 28, 28, 128)  53248       ['conv3_block10_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block10_1_bn (BatchNorma  (None, 28, 28, 128)  512        ['conv3_block10_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv3_block10_1_relu (Activati  (None, 28, 28, 128)  0          ['conv3_block10_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block10_2_conv (Conv2D)  (None, 28, 28, 32)   36864       ['conv3_block10_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block10_concat (Concaten  (None, 28, 28, 448)  0          ['conv3_block9_concat[0][0]',    \n",
            " ate)                                                             'conv3_block10_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block11_0_bn (BatchNorma  (None, 28, 28, 448)  1792       ['conv3_block10_concat[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv3_block11_0_relu (Activati  (None, 28, 28, 448)  0          ['conv3_block11_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block11_1_conv (Conv2D)  (None, 28, 28, 128)  57344       ['conv3_block11_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block11_1_bn (BatchNorma  (None, 28, 28, 128)  512        ['conv3_block11_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv3_block11_1_relu (Activati  (None, 28, 28, 128)  0          ['conv3_block11_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block11_2_conv (Conv2D)  (None, 28, 28, 32)   36864       ['conv3_block11_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block11_concat (Concaten  (None, 28, 28, 480)  0          ['conv3_block10_concat[0][0]',   \n",
            " ate)                                                             'conv3_block11_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block12_0_bn (BatchNorma  (None, 28, 28, 480)  1920       ['conv3_block11_concat[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv3_block12_0_relu (Activati  (None, 28, 28, 480)  0          ['conv3_block12_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block12_1_conv (Conv2D)  (None, 28, 28, 128)  61440       ['conv3_block12_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block12_1_bn (BatchNorma  (None, 28, 28, 128)  512        ['conv3_block12_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv3_block12_1_relu (Activati  (None, 28, 28, 128)  0          ['conv3_block12_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block12_2_conv (Conv2D)  (None, 28, 28, 32)   36864       ['conv3_block12_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block12_concat (Concaten  (None, 28, 28, 512)  0          ['conv3_block11_concat[0][0]',   \n",
            " ate)                                                             'conv3_block12_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " pool3_bn (BatchNormalization)  (None, 28, 28, 512)  2048        ['conv3_block12_concat[0][0]']   \n",
            "                                                                                                  \n",
            " pool3_relu (Activation)        (None, 28, 28, 512)  0           ['pool3_bn[0][0]']               \n",
            "                                                                                                  \n",
            " pool3_conv (Conv2D)            (None, 28, 28, 256)  131072      ['pool3_relu[0][0]']             \n",
            "                                                                                                  \n",
            " pool3_pool (AveragePooling2D)  (None, 14, 14, 256)  0           ['pool3_conv[0][0]']             \n",
            "                                                                                                  \n",
            " conv4_block1_0_bn (BatchNormal  (None, 14, 14, 256)  1024       ['pool3_pool[0][0]']             \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_0_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_1_conv (Conv2D)   (None, 14, 14, 128)  32768       ['conv4_block1_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block1_1_bn (BatchNormal  (None, 14, 14, 128)  512        ['conv4_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_1_relu (Activatio  (None, 14, 14, 128)  0          ['conv4_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_2_conv (Conv2D)   (None, 14, 14, 32)   36864       ['conv4_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block1_concat (Concatena  (None, 14, 14, 288)  0          ['pool3_pool[0][0]',             \n",
            " te)                                                              'conv4_block1_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block2_0_bn (BatchNormal  (None, 14, 14, 288)  1152       ['conv4_block1_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_0_relu (Activatio  (None, 14, 14, 288)  0          ['conv4_block2_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_1_conv (Conv2D)   (None, 14, 14, 128)  36864       ['conv4_block2_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block2_1_bn (BatchNormal  (None, 14, 14, 128)  512        ['conv4_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_1_relu (Activatio  (None, 14, 14, 128)  0          ['conv4_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_2_conv (Conv2D)   (None, 14, 14, 32)   36864       ['conv4_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block2_concat (Concatena  (None, 14, 14, 320)  0          ['conv4_block1_concat[0][0]',    \n",
            " te)                                                              'conv4_block2_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block3_0_bn (BatchNormal  (None, 14, 14, 320)  1280       ['conv4_block2_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_0_relu (Activatio  (None, 14, 14, 320)  0          ['conv4_block3_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_1_conv (Conv2D)   (None, 14, 14, 128)  40960       ['conv4_block3_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block3_1_bn (BatchNormal  (None, 14, 14, 128)  512        ['conv4_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_1_relu (Activatio  (None, 14, 14, 128)  0          ['conv4_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_2_conv (Conv2D)   (None, 14, 14, 32)   36864       ['conv4_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block3_concat (Concatena  (None, 14, 14, 352)  0          ['conv4_block2_concat[0][0]',    \n",
            " te)                                                              'conv4_block3_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block4_0_bn (BatchNormal  (None, 14, 14, 352)  1408       ['conv4_block3_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_0_relu (Activatio  (None, 14, 14, 352)  0          ['conv4_block4_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_1_conv (Conv2D)   (None, 14, 14, 128)  45056       ['conv4_block4_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block4_1_bn (BatchNormal  (None, 14, 14, 128)  512        ['conv4_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_1_relu (Activatio  (None, 14, 14, 128)  0          ['conv4_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_2_conv (Conv2D)   (None, 14, 14, 32)   36864       ['conv4_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block4_concat (Concatena  (None, 14, 14, 384)  0          ['conv4_block3_concat[0][0]',    \n",
            " te)                                                              'conv4_block4_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block5_0_bn (BatchNormal  (None, 14, 14, 384)  1536       ['conv4_block4_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_0_relu (Activatio  (None, 14, 14, 384)  0          ['conv4_block5_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_1_conv (Conv2D)   (None, 14, 14, 128)  49152       ['conv4_block5_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block5_1_bn (BatchNormal  (None, 14, 14, 128)  512        ['conv4_block5_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_1_relu (Activatio  (None, 14, 14, 128)  0          ['conv4_block5_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_2_conv (Conv2D)   (None, 14, 14, 32)   36864       ['conv4_block5_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block5_concat (Concatena  (None, 14, 14, 416)  0          ['conv4_block4_concat[0][0]',    \n",
            " te)                                                              'conv4_block5_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block6_0_bn (BatchNormal  (None, 14, 14, 416)  1664       ['conv4_block5_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_0_relu (Activatio  (None, 14, 14, 416)  0          ['conv4_block6_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_1_conv (Conv2D)   (None, 14, 14, 128)  53248       ['conv4_block6_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block6_1_bn (BatchNormal  (None, 14, 14, 128)  512        ['conv4_block6_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_1_relu (Activatio  (None, 14, 14, 128)  0          ['conv4_block6_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_2_conv (Conv2D)   (None, 14, 14, 32)   36864       ['conv4_block6_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block6_concat (Concatena  (None, 14, 14, 448)  0          ['conv4_block5_concat[0][0]',    \n",
            " te)                                                              'conv4_block6_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block7_0_bn (BatchNormal  (None, 14, 14, 448)  1792       ['conv4_block6_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block7_0_relu (Activatio  (None, 14, 14, 448)  0          ['conv4_block7_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block7_1_conv (Conv2D)   (None, 14, 14, 128)  57344       ['conv4_block7_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block7_1_bn (BatchNormal  (None, 14, 14, 128)  512        ['conv4_block7_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block7_1_relu (Activatio  (None, 14, 14, 128)  0          ['conv4_block7_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block7_2_conv (Conv2D)   (None, 14, 14, 32)   36864       ['conv4_block7_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block7_concat (Concatena  (None, 14, 14, 480)  0          ['conv4_block6_concat[0][0]',    \n",
            " te)                                                              'conv4_block7_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block8_0_bn (BatchNormal  (None, 14, 14, 480)  1920       ['conv4_block7_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block8_0_relu (Activatio  (None, 14, 14, 480)  0          ['conv4_block8_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block8_1_conv (Conv2D)   (None, 14, 14, 128)  61440       ['conv4_block8_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block8_1_bn (BatchNormal  (None, 14, 14, 128)  512        ['conv4_block8_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block8_1_relu (Activatio  (None, 14, 14, 128)  0          ['conv4_block8_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block8_2_conv (Conv2D)   (None, 14, 14, 32)   36864       ['conv4_block8_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block8_concat (Concatena  (None, 14, 14, 512)  0          ['conv4_block7_concat[0][0]',    \n",
            " te)                                                              'conv4_block8_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block9_0_bn (BatchNormal  (None, 14, 14, 512)  2048       ['conv4_block8_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block9_0_relu (Activatio  (None, 14, 14, 512)  0          ['conv4_block9_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block9_1_conv (Conv2D)   (None, 14, 14, 128)  65536       ['conv4_block9_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block9_1_bn (BatchNormal  (None, 14, 14, 128)  512        ['conv4_block9_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block9_1_relu (Activatio  (None, 14, 14, 128)  0          ['conv4_block9_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block9_2_conv (Conv2D)   (None, 14, 14, 32)   36864       ['conv4_block9_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block9_concat (Concatena  (None, 14, 14, 544)  0          ['conv4_block8_concat[0][0]',    \n",
            " te)                                                              'conv4_block9_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block10_0_bn (BatchNorma  (None, 14, 14, 544)  2176       ['conv4_block9_concat[0][0]']    \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block10_0_relu (Activati  (None, 14, 14, 544)  0          ['conv4_block10_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block10_1_conv (Conv2D)  (None, 14, 14, 128)  69632       ['conv4_block10_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block10_1_bn (BatchNorma  (None, 14, 14, 128)  512        ['conv4_block10_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block10_1_relu (Activati  (None, 14, 14, 128)  0          ['conv4_block10_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block10_2_conv (Conv2D)  (None, 14, 14, 32)   36864       ['conv4_block10_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block10_concat (Concaten  (None, 14, 14, 576)  0          ['conv4_block9_concat[0][0]',    \n",
            " ate)                                                             'conv4_block10_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block11_0_bn (BatchNorma  (None, 14, 14, 576)  2304       ['conv4_block10_concat[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block11_0_relu (Activati  (None, 14, 14, 576)  0          ['conv4_block11_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block11_1_conv (Conv2D)  (None, 14, 14, 128)  73728       ['conv4_block11_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block11_1_bn (BatchNorma  (None, 14, 14, 128)  512        ['conv4_block11_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block11_1_relu (Activati  (None, 14, 14, 128)  0          ['conv4_block11_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block11_2_conv (Conv2D)  (None, 14, 14, 32)   36864       ['conv4_block11_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block11_concat (Concaten  (None, 14, 14, 608)  0          ['conv4_block10_concat[0][0]',   \n",
            " ate)                                                             'conv4_block11_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block12_0_bn (BatchNorma  (None, 14, 14, 608)  2432       ['conv4_block11_concat[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block12_0_relu (Activati  (None, 14, 14, 608)  0          ['conv4_block12_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block12_1_conv (Conv2D)  (None, 14, 14, 128)  77824       ['conv4_block12_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block12_1_bn (BatchNorma  (None, 14, 14, 128)  512        ['conv4_block12_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block12_1_relu (Activati  (None, 14, 14, 128)  0          ['conv4_block12_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block12_2_conv (Conv2D)  (None, 14, 14, 32)   36864       ['conv4_block12_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block12_concat (Concaten  (None, 14, 14, 640)  0          ['conv4_block11_concat[0][0]',   \n",
            " ate)                                                             'conv4_block12_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block13_0_bn (BatchNorma  (None, 14, 14, 640)  2560       ['conv4_block12_concat[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block13_0_relu (Activati  (None, 14, 14, 640)  0          ['conv4_block13_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block13_1_conv (Conv2D)  (None, 14, 14, 128)  81920       ['conv4_block13_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block13_1_bn (BatchNorma  (None, 14, 14, 128)  512        ['conv4_block13_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block13_1_relu (Activati  (None, 14, 14, 128)  0          ['conv4_block13_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block13_2_conv (Conv2D)  (None, 14, 14, 32)   36864       ['conv4_block13_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block13_concat (Concaten  (None, 14, 14, 672)  0          ['conv4_block12_concat[0][0]',   \n",
            " ate)                                                             'conv4_block13_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block14_0_bn (BatchNorma  (None, 14, 14, 672)  2688       ['conv4_block13_concat[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block14_0_relu (Activati  (None, 14, 14, 672)  0          ['conv4_block14_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block14_1_conv (Conv2D)  (None, 14, 14, 128)  86016       ['conv4_block14_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block14_1_bn (BatchNorma  (None, 14, 14, 128)  512        ['conv4_block14_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block14_1_relu (Activati  (None, 14, 14, 128)  0          ['conv4_block14_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block14_2_conv (Conv2D)  (None, 14, 14, 32)   36864       ['conv4_block14_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block14_concat (Concaten  (None, 14, 14, 704)  0          ['conv4_block13_concat[0][0]',   \n",
            " ate)                                                             'conv4_block14_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block15_0_bn (BatchNorma  (None, 14, 14, 704)  2816       ['conv4_block14_concat[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block15_0_relu (Activati  (None, 14, 14, 704)  0          ['conv4_block15_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block15_1_conv (Conv2D)  (None, 14, 14, 128)  90112       ['conv4_block15_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block15_1_bn (BatchNorma  (None, 14, 14, 128)  512        ['conv4_block15_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block15_1_relu (Activati  (None, 14, 14, 128)  0          ['conv4_block15_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block15_2_conv (Conv2D)  (None, 14, 14, 32)   36864       ['conv4_block15_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block15_concat (Concaten  (None, 14, 14, 736)  0          ['conv4_block14_concat[0][0]',   \n",
            " ate)                                                             'conv4_block15_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block16_0_bn (BatchNorma  (None, 14, 14, 736)  2944       ['conv4_block15_concat[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block16_0_relu (Activati  (None, 14, 14, 736)  0          ['conv4_block16_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block16_1_conv (Conv2D)  (None, 14, 14, 128)  94208       ['conv4_block16_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block16_1_bn (BatchNorma  (None, 14, 14, 128)  512        ['conv4_block16_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block16_1_relu (Activati  (None, 14, 14, 128)  0          ['conv4_block16_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block16_2_conv (Conv2D)  (None, 14, 14, 32)   36864       ['conv4_block16_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block16_concat (Concaten  (None, 14, 14, 768)  0          ['conv4_block15_concat[0][0]',   \n",
            " ate)                                                             'conv4_block16_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block17_0_bn (BatchNorma  (None, 14, 14, 768)  3072       ['conv4_block16_concat[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block17_0_relu (Activati  (None, 14, 14, 768)  0          ['conv4_block17_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block17_1_conv (Conv2D)  (None, 14, 14, 128)  98304       ['conv4_block17_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block17_1_bn (BatchNorma  (None, 14, 14, 128)  512        ['conv4_block17_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block17_1_relu (Activati  (None, 14, 14, 128)  0          ['conv4_block17_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block17_2_conv (Conv2D)  (None, 14, 14, 32)   36864       ['conv4_block17_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block17_concat (Concaten  (None, 14, 14, 800)  0          ['conv4_block16_concat[0][0]',   \n",
            " ate)                                                             'conv4_block17_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block18_0_bn (BatchNorma  (None, 14, 14, 800)  3200       ['conv4_block17_concat[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block18_0_relu (Activati  (None, 14, 14, 800)  0          ['conv4_block18_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block18_1_conv (Conv2D)  (None, 14, 14, 128)  102400      ['conv4_block18_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block18_1_bn (BatchNorma  (None, 14, 14, 128)  512        ['conv4_block18_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block18_1_relu (Activati  (None, 14, 14, 128)  0          ['conv4_block18_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block18_2_conv (Conv2D)  (None, 14, 14, 32)   36864       ['conv4_block18_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block18_concat (Concaten  (None, 14, 14, 832)  0          ['conv4_block17_concat[0][0]',   \n",
            " ate)                                                             'conv4_block18_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block19_0_bn (BatchNorma  (None, 14, 14, 832)  3328       ['conv4_block18_concat[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block19_0_relu (Activati  (None, 14, 14, 832)  0          ['conv4_block19_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block19_1_conv (Conv2D)  (None, 14, 14, 128)  106496      ['conv4_block19_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block19_1_bn (BatchNorma  (None, 14, 14, 128)  512        ['conv4_block19_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block19_1_relu (Activati  (None, 14, 14, 128)  0          ['conv4_block19_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block19_2_conv (Conv2D)  (None, 14, 14, 32)   36864       ['conv4_block19_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block19_concat (Concaten  (None, 14, 14, 864)  0          ['conv4_block18_concat[0][0]',   \n",
            " ate)                                                             'conv4_block19_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block20_0_bn (BatchNorma  (None, 14, 14, 864)  3456       ['conv4_block19_concat[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block20_0_relu (Activati  (None, 14, 14, 864)  0          ['conv4_block20_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block20_1_conv (Conv2D)  (None, 14, 14, 128)  110592      ['conv4_block20_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block20_1_bn (BatchNorma  (None, 14, 14, 128)  512        ['conv4_block20_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block20_1_relu (Activati  (None, 14, 14, 128)  0          ['conv4_block20_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block20_2_conv (Conv2D)  (None, 14, 14, 32)   36864       ['conv4_block20_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block20_concat (Concaten  (None, 14, 14, 896)  0          ['conv4_block19_concat[0][0]',   \n",
            " ate)                                                             'conv4_block20_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block21_0_bn (BatchNorma  (None, 14, 14, 896)  3584       ['conv4_block20_concat[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block21_0_relu (Activati  (None, 14, 14, 896)  0          ['conv4_block21_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block21_1_conv (Conv2D)  (None, 14, 14, 128)  114688      ['conv4_block21_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block21_1_bn (BatchNorma  (None, 14, 14, 128)  512        ['conv4_block21_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block21_1_relu (Activati  (None, 14, 14, 128)  0          ['conv4_block21_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block21_2_conv (Conv2D)  (None, 14, 14, 32)   36864       ['conv4_block21_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block21_concat (Concaten  (None, 14, 14, 928)  0          ['conv4_block20_concat[0][0]',   \n",
            " ate)                                                             'conv4_block21_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block22_0_bn (BatchNorma  (None, 14, 14, 928)  3712       ['conv4_block21_concat[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block22_0_relu (Activati  (None, 14, 14, 928)  0          ['conv4_block22_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block22_1_conv (Conv2D)  (None, 14, 14, 128)  118784      ['conv4_block22_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block22_1_bn (BatchNorma  (None, 14, 14, 128)  512        ['conv4_block22_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block22_1_relu (Activati  (None, 14, 14, 128)  0          ['conv4_block22_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block22_2_conv (Conv2D)  (None, 14, 14, 32)   36864       ['conv4_block22_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block22_concat (Concaten  (None, 14, 14, 960)  0          ['conv4_block21_concat[0][0]',   \n",
            " ate)                                                             'conv4_block22_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block23_0_bn (BatchNorma  (None, 14, 14, 960)  3840       ['conv4_block22_concat[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block23_0_relu (Activati  (None, 14, 14, 960)  0          ['conv4_block23_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block23_1_conv (Conv2D)  (None, 14, 14, 128)  122880      ['conv4_block23_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block23_1_bn (BatchNorma  (None, 14, 14, 128)  512        ['conv4_block23_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block23_1_relu (Activati  (None, 14, 14, 128)  0          ['conv4_block23_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block23_2_conv (Conv2D)  (None, 14, 14, 32)   36864       ['conv4_block23_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block23_concat (Concaten  (None, 14, 14, 992)  0          ['conv4_block22_concat[0][0]',   \n",
            " ate)                                                             'conv4_block23_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block24_0_bn (BatchNorma  (None, 14, 14, 992)  3968       ['conv4_block23_concat[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block24_0_relu (Activati  (None, 14, 14, 992)  0          ['conv4_block24_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block24_1_conv (Conv2D)  (None, 14, 14, 128)  126976      ['conv4_block24_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block24_1_bn (BatchNorma  (None, 14, 14, 128)  512        ['conv4_block24_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block24_1_relu (Activati  (None, 14, 14, 128)  0          ['conv4_block24_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block24_2_conv (Conv2D)  (None, 14, 14, 32)   36864       ['conv4_block24_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block24_concat (Concaten  (None, 14, 14, 1024  0          ['conv4_block23_concat[0][0]',   \n",
            " ate)                           )                                 'conv4_block24_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " pool4_bn (BatchNormalization)  (None, 14, 14, 1024  4096        ['conv4_block24_concat[0][0]']   \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " pool4_relu (Activation)        (None, 14, 14, 1024  0           ['pool4_bn[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " pool4_conv (Conv2D)            (None, 14, 14, 512)  524288      ['pool4_relu[0][0]']             \n",
            "                                                                                                  \n",
            " pool4_pool (AveragePooling2D)  (None, 7, 7, 512)    0           ['pool4_conv[0][0]']             \n",
            "                                                                                                  \n",
            " conv5_block1_0_bn (BatchNormal  (None, 7, 7, 512)   2048        ['pool4_pool[0][0]']             \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_0_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_1_conv (Conv2D)   (None, 7, 7, 128)    65536       ['conv5_block1_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block1_1_bn (BatchNormal  (None, 7, 7, 128)   512         ['conv5_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_1_relu (Activatio  (None, 7, 7, 128)   0           ['conv5_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_2_conv (Conv2D)   (None, 7, 7, 32)     36864       ['conv5_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block1_concat (Concatena  (None, 7, 7, 544)   0           ['pool4_pool[0][0]',             \n",
            " te)                                                              'conv5_block1_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block2_0_bn (BatchNormal  (None, 7, 7, 544)   2176        ['conv5_block1_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_0_relu (Activatio  (None, 7, 7, 544)   0           ['conv5_block2_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_1_conv (Conv2D)   (None, 7, 7, 128)    69632       ['conv5_block2_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block2_1_bn (BatchNormal  (None, 7, 7, 128)   512         ['conv5_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_1_relu (Activatio  (None, 7, 7, 128)   0           ['conv5_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_2_conv (Conv2D)   (None, 7, 7, 32)     36864       ['conv5_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block2_concat (Concatena  (None, 7, 7, 576)   0           ['conv5_block1_concat[0][0]',    \n",
            " te)                                                              'conv5_block2_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block3_0_bn (BatchNormal  (None, 7, 7, 576)   2304        ['conv5_block2_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_0_relu (Activatio  (None, 7, 7, 576)   0           ['conv5_block3_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_1_conv (Conv2D)   (None, 7, 7, 128)    73728       ['conv5_block3_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block3_1_bn (BatchNormal  (None, 7, 7, 128)   512         ['conv5_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_1_relu (Activatio  (None, 7, 7, 128)   0           ['conv5_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_2_conv (Conv2D)   (None, 7, 7, 32)     36864       ['conv5_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block3_concat (Concatena  (None, 7, 7, 608)   0           ['conv5_block2_concat[0][0]',    \n",
            " te)                                                              'conv5_block3_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block4_0_bn (BatchNormal  (None, 7, 7, 608)   2432        ['conv5_block3_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block4_0_relu (Activatio  (None, 7, 7, 608)   0           ['conv5_block4_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block4_1_conv (Conv2D)   (None, 7, 7, 128)    77824       ['conv5_block4_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block4_1_bn (BatchNormal  (None, 7, 7, 128)   512         ['conv5_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block4_1_relu (Activatio  (None, 7, 7, 128)   0           ['conv5_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block4_2_conv (Conv2D)   (None, 7, 7, 32)     36864       ['conv5_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block4_concat (Concatena  (None, 7, 7, 640)   0           ['conv5_block3_concat[0][0]',    \n",
            " te)                                                              'conv5_block4_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block5_0_bn (BatchNormal  (None, 7, 7, 640)   2560        ['conv5_block4_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block5_0_relu (Activatio  (None, 7, 7, 640)   0           ['conv5_block5_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block5_1_conv (Conv2D)   (None, 7, 7, 128)    81920       ['conv5_block5_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block5_1_bn (BatchNormal  (None, 7, 7, 128)   512         ['conv5_block5_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block5_1_relu (Activatio  (None, 7, 7, 128)   0           ['conv5_block5_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block5_2_conv (Conv2D)   (None, 7, 7, 32)     36864       ['conv5_block5_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block5_concat (Concatena  (None, 7, 7, 672)   0           ['conv5_block4_concat[0][0]',    \n",
            " te)                                                              'conv5_block5_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block6_0_bn (BatchNormal  (None, 7, 7, 672)   2688        ['conv5_block5_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block6_0_relu (Activatio  (None, 7, 7, 672)   0           ['conv5_block6_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block6_1_conv (Conv2D)   (None, 7, 7, 128)    86016       ['conv5_block6_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block6_1_bn (BatchNormal  (None, 7, 7, 128)   512         ['conv5_block6_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block6_1_relu (Activatio  (None, 7, 7, 128)   0           ['conv5_block6_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block6_2_conv (Conv2D)   (None, 7, 7, 32)     36864       ['conv5_block6_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block6_concat (Concatena  (None, 7, 7, 704)   0           ['conv5_block5_concat[0][0]',    \n",
            " te)                                                              'conv5_block6_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block7_0_bn (BatchNormal  (None, 7, 7, 704)   2816        ['conv5_block6_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block7_0_relu (Activatio  (None, 7, 7, 704)   0           ['conv5_block7_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block7_1_conv (Conv2D)   (None, 7, 7, 128)    90112       ['conv5_block7_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block7_1_bn (BatchNormal  (None, 7, 7, 128)   512         ['conv5_block7_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block7_1_relu (Activatio  (None, 7, 7, 128)   0           ['conv5_block7_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block7_2_conv (Conv2D)   (None, 7, 7, 32)     36864       ['conv5_block7_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block7_concat (Concatena  (None, 7, 7, 736)   0           ['conv5_block6_concat[0][0]',    \n",
            " te)                                                              'conv5_block7_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block8_0_bn (BatchNormal  (None, 7, 7, 736)   2944        ['conv5_block7_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block8_0_relu (Activatio  (None, 7, 7, 736)   0           ['conv5_block8_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block8_1_conv (Conv2D)   (None, 7, 7, 128)    94208       ['conv5_block8_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block8_1_bn (BatchNormal  (None, 7, 7, 128)   512         ['conv5_block8_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block8_1_relu (Activatio  (None, 7, 7, 128)   0           ['conv5_block8_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block8_2_conv (Conv2D)   (None, 7, 7, 32)     36864       ['conv5_block8_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block8_concat (Concatena  (None, 7, 7, 768)   0           ['conv5_block7_concat[0][0]',    \n",
            " te)                                                              'conv5_block8_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block9_0_bn (BatchNormal  (None, 7, 7, 768)   3072        ['conv5_block8_concat[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block9_0_relu (Activatio  (None, 7, 7, 768)   0           ['conv5_block9_0_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block9_1_conv (Conv2D)   (None, 7, 7, 128)    98304       ['conv5_block9_0_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block9_1_bn (BatchNormal  (None, 7, 7, 128)   512         ['conv5_block9_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block9_1_relu (Activatio  (None, 7, 7, 128)   0           ['conv5_block9_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block9_2_conv (Conv2D)   (None, 7, 7, 32)     36864       ['conv5_block9_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block9_concat (Concatena  (None, 7, 7, 800)   0           ['conv5_block8_concat[0][0]',    \n",
            " te)                                                              'conv5_block9_2_conv[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block10_0_bn (BatchNorma  (None, 7, 7, 800)   3200        ['conv5_block9_concat[0][0]']    \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv5_block10_0_relu (Activati  (None, 7, 7, 800)   0           ['conv5_block10_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block10_1_conv (Conv2D)  (None, 7, 7, 128)    102400      ['conv5_block10_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block10_1_bn (BatchNorma  (None, 7, 7, 128)   512         ['conv5_block10_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv5_block10_1_relu (Activati  (None, 7, 7, 128)   0           ['conv5_block10_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block10_2_conv (Conv2D)  (None, 7, 7, 32)     36864       ['conv5_block10_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block10_concat (Concaten  (None, 7, 7, 832)   0           ['conv5_block9_concat[0][0]',    \n",
            " ate)                                                             'conv5_block10_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block11_0_bn (BatchNorma  (None, 7, 7, 832)   3328        ['conv5_block10_concat[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv5_block11_0_relu (Activati  (None, 7, 7, 832)   0           ['conv5_block11_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block11_1_conv (Conv2D)  (None, 7, 7, 128)    106496      ['conv5_block11_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block11_1_bn (BatchNorma  (None, 7, 7, 128)   512         ['conv5_block11_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv5_block11_1_relu (Activati  (None, 7, 7, 128)   0           ['conv5_block11_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block11_2_conv (Conv2D)  (None, 7, 7, 32)     36864       ['conv5_block11_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block11_concat (Concaten  (None, 7, 7, 864)   0           ['conv5_block10_concat[0][0]',   \n",
            " ate)                                                             'conv5_block11_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block12_0_bn (BatchNorma  (None, 7, 7, 864)   3456        ['conv5_block11_concat[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv5_block12_0_relu (Activati  (None, 7, 7, 864)   0           ['conv5_block12_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block12_1_conv (Conv2D)  (None, 7, 7, 128)    110592      ['conv5_block12_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block12_1_bn (BatchNorma  (None, 7, 7, 128)   512         ['conv5_block12_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv5_block12_1_relu (Activati  (None, 7, 7, 128)   0           ['conv5_block12_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block12_2_conv (Conv2D)  (None, 7, 7, 32)     36864       ['conv5_block12_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block12_concat (Concaten  (None, 7, 7, 896)   0           ['conv5_block11_concat[0][0]',   \n",
            " ate)                                                             'conv5_block12_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block13_0_bn (BatchNorma  (None, 7, 7, 896)   3584        ['conv5_block12_concat[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv5_block13_0_relu (Activati  (None, 7, 7, 896)   0           ['conv5_block13_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block13_1_conv (Conv2D)  (None, 7, 7, 128)    114688      ['conv5_block13_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block13_1_bn (BatchNorma  (None, 7, 7, 128)   512         ['conv5_block13_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv5_block13_1_relu (Activati  (None, 7, 7, 128)   0           ['conv5_block13_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block13_2_conv (Conv2D)  (None, 7, 7, 32)     36864       ['conv5_block13_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block13_concat (Concaten  (None, 7, 7, 928)   0           ['conv5_block12_concat[0][0]',   \n",
            " ate)                                                             'conv5_block13_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block14_0_bn (BatchNorma  (None, 7, 7, 928)   3712        ['conv5_block13_concat[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv5_block14_0_relu (Activati  (None, 7, 7, 928)   0           ['conv5_block14_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block14_1_conv (Conv2D)  (None, 7, 7, 128)    118784      ['conv5_block14_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block14_1_bn (BatchNorma  (None, 7, 7, 128)   512         ['conv5_block14_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv5_block14_1_relu (Activati  (None, 7, 7, 128)   0           ['conv5_block14_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block14_2_conv (Conv2D)  (None, 7, 7, 32)     36864       ['conv5_block14_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block14_concat (Concaten  (None, 7, 7, 960)   0           ['conv5_block13_concat[0][0]',   \n",
            " ate)                                                             'conv5_block14_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block15_0_bn (BatchNorma  (None, 7, 7, 960)   3840        ['conv5_block14_concat[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv5_block15_0_relu (Activati  (None, 7, 7, 960)   0           ['conv5_block15_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block15_1_conv (Conv2D)  (None, 7, 7, 128)    122880      ['conv5_block15_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block15_1_bn (BatchNorma  (None, 7, 7, 128)   512         ['conv5_block15_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv5_block15_1_relu (Activati  (None, 7, 7, 128)   0           ['conv5_block15_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block15_2_conv (Conv2D)  (None, 7, 7, 32)     36864       ['conv5_block15_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block15_concat (Concaten  (None, 7, 7, 992)   0           ['conv5_block14_concat[0][0]',   \n",
            " ate)                                                             'conv5_block15_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block16_0_bn (BatchNorma  (None, 7, 7, 992)   3968        ['conv5_block15_concat[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv5_block16_0_relu (Activati  (None, 7, 7, 992)   0           ['conv5_block16_0_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block16_1_conv (Conv2D)  (None, 7, 7, 128)    126976      ['conv5_block16_0_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block16_1_bn (BatchNorma  (None, 7, 7, 128)   512         ['conv5_block16_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv5_block16_1_relu (Activati  (None, 7, 7, 128)   0           ['conv5_block16_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block16_2_conv (Conv2D)  (None, 7, 7, 32)     36864       ['conv5_block16_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block16_concat (Concaten  (None, 7, 7, 1024)  0           ['conv5_block15_concat[0][0]',   \n",
            " ate)                                                             'conv5_block16_2_conv[0][0]']   \n",
            "                                                                                                  \n",
            " bn (BatchNormalization)        (None, 7, 7, 1024)   4096        ['conv5_block16_concat[0][0]']   \n",
            "                                                                                                  \n",
            " relu (Activation)              (None, 7, 7, 1024)   0           ['bn[0][0]']                     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7,037,504\n",
            "Trainable params: 4,777,984\n",
            "Non-trainable params: 2,259,520\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "base_model = base_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnEB0mcGJ3XK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3676da7-7688-4c69-c330-1cab03e140ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "#### LOADING CIFAR10 DATASET\n",
        "\n",
        "def load_data():\n",
        "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test = x_test.astype('float32')\n",
        "\n",
        "  # Convert class vectors to binary class matrices.\n",
        "  y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "  y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train[:],y_train[:]))\n",
        "  test_ds = tf.data.Dataset.from_tensor_slices((x_test[:2000], y_test[:2000]))\n",
        "  train_ds = train_ds.map(lambda x,y: (preprocess_data(x,y)))\n",
        "  test_ds = test_ds.map(lambda x,y: (preprocess_data(x,y)))\n",
        "  batch_size = 100\n",
        "\n",
        "  train_ds = train_ds.cache().batch(batch_size).prefetch(buffer_size=1000)\n",
        "  test_ds = test_ds.cache().batch(batch_size).prefetch(buffer_size=1000)\n",
        "  return train_ds, test_ds\n",
        "\n",
        "train_ds, test_ds = load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC3wWiSXKQwD"
      },
      "source": [
        "## Let's start by training the classical image classifier composed of a feature extractor (DCNN) and a classifier (neural net).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbcviGs5Kjdy"
      },
      "outputs": [],
      "source": [
        "#### LOADING AND TRAINING THE COMPARATIVE MODEL\n",
        "compare_model = tf.keras.Sequential()\n",
        "compare_model.add(base_model)\n",
        "compare_model.add(tf.keras.layers.Dropout(0.3))\n",
        "compare_model.add(tf.keras.layers.Dense(10, activation = 'softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icO6wVqoKrQT"
      },
      "outputs": [],
      "source": [
        "compare_model.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'], optimizer = 'adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrbYmKKmK7EN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebf68b09-cf7c-4ed9-894e-bf13a678a5dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "500/500 [==============================] - 254s 462ms/step - loss: 0.3369 - accuracy: 0.8863 - val_loss: 0.3075 - val_accuracy: 0.8920\n",
            "Epoch 2/30\n",
            "500/500 [==============================] - 234s 468ms/step - loss: 0.1578 - accuracy: 0.9459 - val_loss: 0.2991 - val_accuracy: 0.9085\n",
            "Epoch 3/30\n",
            "500/500 [==============================] - 234s 468ms/step - loss: 0.1004 - accuracy: 0.9649 - val_loss: 0.2922 - val_accuracy: 0.9215\n",
            "Epoch 4/30\n",
            "500/500 [==============================] - 233s 466ms/step - loss: 0.0790 - accuracy: 0.9721 - val_loss: 0.3584 - val_accuracy: 0.9065\n",
            "Epoch 5/30\n",
            "500/500 [==============================] - 234s 467ms/step - loss: 0.0625 - accuracy: 0.9780 - val_loss: 0.2109 - val_accuracy: 0.9410\n",
            "Epoch 6/30\n",
            "500/500 [==============================] - 233s 467ms/step - loss: 0.0465 - accuracy: 0.9841 - val_loss: 0.3332 - val_accuracy: 0.9240\n",
            "Epoch 7/30\n",
            "500/500 [==============================] - 233s 466ms/step - loss: 0.0458 - accuracy: 0.9841 - val_loss: 0.2855 - val_accuracy: 0.9215\n",
            "Epoch 8/30\n",
            "500/500 [==============================] - 233s 466ms/step - loss: 0.0357 - accuracy: 0.9874 - val_loss: 0.2609 - val_accuracy: 0.9330\n",
            "Epoch 9/30\n",
            "500/500 [==============================] - 233s 465ms/step - loss: 0.0369 - accuracy: 0.9871 - val_loss: 0.2936 - val_accuracy: 0.9325\n",
            "Epoch 10/30\n",
            "500/500 [==============================] - 233s 466ms/step - loss: 0.0327 - accuracy: 0.9889 - val_loss: 0.4419 - val_accuracy: 0.9135\n",
            "Epoch 11/30\n",
            "500/500 [==============================] - 232s 465ms/step - loss: 0.0344 - accuracy: 0.9887 - val_loss: 0.3142 - val_accuracy: 0.9305\n",
            "Epoch 12/30\n",
            "500/500 [==============================] - 233s 465ms/step - loss: 0.0285 - accuracy: 0.9903 - val_loss: 0.2608 - val_accuracy: 0.9345\n",
            "Epoch 13/30\n",
            "500/500 [==============================] - 232s 465ms/step - loss: 0.0290 - accuracy: 0.9903 - val_loss: 0.3370 - val_accuracy: 0.9265\n",
            "Epoch 14/30\n",
            "500/500 [==============================] - 233s 465ms/step - loss: 0.0204 - accuracy: 0.9932 - val_loss: 0.2800 - val_accuracy: 0.9380\n",
            "Epoch 15/30\n",
            "500/500 [==============================] - 232s 465ms/step - loss: 0.0210 - accuracy: 0.9930 - val_loss: 0.3401 - val_accuracy: 0.9300\n",
            "Epoch 16/30\n",
            "500/500 [==============================] - 233s 465ms/step - loss: 0.0231 - accuracy: 0.9922 - val_loss: 0.2717 - val_accuracy: 0.9355\n",
            "Epoch 17/30\n",
            "500/500 [==============================] - 233s 465ms/step - loss: 0.0193 - accuracy: 0.9933 - val_loss: 0.3688 - val_accuracy: 0.9285\n",
            "Epoch 18/30\n",
            "500/500 [==============================] - 233s 465ms/step - loss: 0.0205 - accuracy: 0.9933 - val_loss: 0.2751 - val_accuracy: 0.9410\n",
            "Epoch 19/30\n",
            "500/500 [==============================] - 233s 465ms/step - loss: 0.0180 - accuracy: 0.9939 - val_loss: 0.3086 - val_accuracy: 0.9410\n",
            "Epoch 20/30\n",
            "500/500 [==============================] - 233s 465ms/step - loss: 0.0193 - accuracy: 0.9938 - val_loss: 0.3632 - val_accuracy: 0.9310\n",
            "Epoch 21/30\n",
            "500/500 [==============================] - 233s 465ms/step - loss: 0.0178 - accuracy: 0.9939 - val_loss: 0.3066 - val_accuracy: 0.9370\n",
            "Epoch 22/30\n",
            "500/500 [==============================] - 232s 465ms/step - loss: 0.0165 - accuracy: 0.9946 - val_loss: 0.2721 - val_accuracy: 0.9370\n",
            "Epoch 23/30\n",
            "500/500 [==============================] - 233s 465ms/step - loss: 0.0143 - accuracy: 0.9951 - val_loss: 0.3264 - val_accuracy: 0.9350\n",
            "Epoch 24/30\n",
            "500/500 [==============================] - 231s 461ms/step - loss: 0.0110 - accuracy: 0.9964 - val_loss: 0.2624 - val_accuracy: 0.9460\n",
            "Epoch 25/30\n",
            "500/500 [==============================] - 231s 461ms/step - loss: 0.0208 - accuracy: 0.9935 - val_loss: 0.2701 - val_accuracy: 0.9450\n",
            "Epoch 26/30\n",
            "500/500 [==============================] - 231s 461ms/step - loss: 0.0165 - accuracy: 0.9947 - val_loss: 0.2922 - val_accuracy: 0.9400\n",
            "Epoch 27/30\n",
            "500/500 [==============================] - 228s 456ms/step - loss: 0.0102 - accuracy: 0.9969 - val_loss: 0.2819 - val_accuracy: 0.9430\n",
            "Epoch 28/30\n",
            "500/500 [==============================] - 228s 455ms/step - loss: 0.0118 - accuracy: 0.9964 - val_loss: 0.2461 - val_accuracy: 0.9430\n",
            "Epoch 29/30\n",
            "500/500 [==============================] - 230s 461ms/step - loss: 0.0180 - accuracy: 0.9940 - val_loss: 0.3106 - val_accuracy: 0.9345\n",
            "Epoch 30/30\n",
            "500/500 [==============================] - 228s 455ms/step - loss: 0.0103 - accuracy: 0.9964 - val_loss: 0.3694 - val_accuracy: 0.9345\n"
          ]
        }
      ],
      "source": [
        "training1 = compare_model.fit(train_ds, epochs = 30, verbose=1, validation_data = test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNiS1vTDLKAc"
      },
      "source": [
        "## We then freeze the DCNN to preserve the feature space and generate the feature maps for the dataset at hand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8lQcPBJLWrN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d5ce03e-3bb3-45f4-93eb-90be1d5ad1cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500/500 [==============================] - 134s 263ms/step\n"
          ]
        }
      ],
      "source": [
        "base_model.trainable = False\n",
        "feat_maps_cifar = base_model.predict(train_ds, verbose = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvXxm3y_Mh2l"
      },
      "source": [
        "## Variational Feature Encoder\n",
        "This section uses a variational autoencoder composed only of dense layers to perform dimensionality reduction of feature maps. It is trained to reconstruct feature maps from CIFAR10 dataset. Once it is trained we use the encoded representation in the latent space to feed a new classifier. This new system is now composed of the frozen DCNN, the encoder part of the variational autoencoder (VFE) and the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6pPJ0zLNuq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f833a38-d0b9-4068-fc59-ad6edc4e15b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)          [(None, 1024)]       0           []                               \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 256)          262400      ['input_11[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 256)         1024        ['dense_12[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " z_mean (Dense)                 (None, 256)          65792       ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " z_log_var (Dense)              (None, 256)          65792       ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " sampling_3 (Sampling)          (None, 256)          0           ['z_mean[0][0]',                 \n",
            "                                                                  'z_log_var[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 395,008\n",
            "Trainable params: 394,496\n",
            "Non-trainable params: 512\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_12 (InputLayer)       [(None, 256)]             0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 512)               131584    \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 512)              2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1024)              525312    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 658,944\n",
            "Trainable params: 657,920\n",
            "Non-trainable params: 1,024\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#### CREATING THE VARIATIONAL AUTOENCODER\n",
        "\n",
        "## THE SAMPLING BLOCK TAKES THE OUTPUT OF THE MEAN AND VARIANCE BLOCKS.\n",
        "## IT THEN PROVIDES A DISTRIBUTION TO THE DECODER WHICH HAS THE TASK OF RECONSTRUCTING THE FEATURE MAPS.\n",
        "\n",
        "\n",
        "class Sampling(tf.keras.layers.Layer):\n",
        "  \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "  def call(self, inputs):\n",
        "      z_mean, z_log_var = inputs\n",
        "      batch = tf.shape(z_mean)[0]\n",
        "      dim = tf.shape(z_mean)[1]\n",
        "      epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "      return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "## DIMENSIONALITY OF THE LATENT SPACE\n",
        "latent_dim = 256\n",
        "\n",
        "encoder_inputs = tf.keras.Input(shape=(1024,))\n",
        "x = tf.keras.layers.Dense(256, activation = 'relu')(encoder_inputs)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "z_mean = tf.keras.layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = tf.keras.layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "encoder = tf.keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()\n",
        "\n",
        "latent_inputs = tf.keras.Input(shape=(latent_dim,))\n",
        "x = tf.keras.layers.Dense(512, activation='relu')(latent_inputs)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "decoder_outputs = tf.keras.layers.Dense(1024, activation=\"relu\")(x)\n",
        "decoder = tf.keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "decoder.summary()\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "## DEFINING THE LOSSES TO BE USED IN THE TRAINING PROCESS\n",
        "## MEAN SQUARRED ERROR\n",
        "def mse_loss(y_true, y_pred):\n",
        "\n",
        "    r_loss = tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "    ## MULTIPLICATIVE FACTOR ON THE TOTAL LOSS (1000)\n",
        "    return 1000*r_loss\n",
        "\n",
        "##Kullblack-Leibler Divergence Factor\n",
        "def kl_loss(mean, log_var):\n",
        "\n",
        "    kl_loss =  -0.5 * (1 + log_var - tf.square(mean) - tf.exp(log_var))\n",
        "    ## BETA = 1\n",
        "    return 1*kl_loss\n",
        "\n",
        "## TOTAL LOSS\n",
        "def vae_loss(y_true, y_pred, mean, var):\n",
        "\n",
        "    r_loss = mse_loss(y_true, y_pred)\n",
        "    kl_loss_ = kl_loss(mean, var)\n",
        "    return  r_loss + kl_loss_\n",
        "\n",
        "\n",
        "\n",
        "## DEFINING THE TRAINING PROCESS AND THE OBJECTIVE FUNCTION TO CALCULATE THE LOSS\n",
        "\n",
        "class VAE(tf.keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = mse_loss(data, reconstruction)\n",
        "            kl_loss_ = kl_loss(z_mean, z_log_var)\n",
        "            total_loss = vae_loss(data, reconstruction, z_mean, z_log_var)\n",
        "\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss_)\n",
        "\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTb-5oajPqH0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b9b1343d-53ff-44db-9767-11b194dde7e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 103.8051 - reconstruction_loss: 40.9204 - kl_loss: 2.6784\n",
            "Epoch 2/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 14.7844 - reconstruction_loss: 10.9683 - kl_loss: 2.6964\n",
            "Epoch 3/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 11.5390 - reconstruction_loss: 8.3653 - kl_loss: 2.6439\n",
            "Epoch 4/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 10.0894 - reconstruction_loss: 7.2095 - kl_loss: 2.6177\n",
            "Epoch 5/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 9.1049 - reconstruction_loss: 6.4170 - kl_loss: 2.5973\n",
            "Epoch 6/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 8.6451 - reconstruction_loss: 5.8684 - kl_loss: 2.5870\n",
            "Epoch 7/2000\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 8.3293 - reconstruction_loss: 5.5687 - kl_loss: 2.5704\n",
            "Epoch 8/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 7.8884 - reconstruction_loss: 5.2580 - kl_loss: 2.5554\n",
            "Epoch 9/2000\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 7.7251 - reconstruction_loss: 5.0716 - kl_loss: 2.5388\n",
            "Epoch 10/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 7.4064 - reconstruction_loss: 4.8272 - kl_loss: 2.5221\n",
            "Epoch 11/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 7.3728 - reconstruction_loss: 4.7423 - kl_loss: 2.5052\n",
            "Epoch 12/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 7.2625 - reconstruction_loss: 4.6535 - kl_loss: 2.4866\n",
            "Epoch 13/2000\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 6.9094 - reconstruction_loss: 4.4825 - kl_loss: 2.4670\n",
            "Epoch 14/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 6.8674 - reconstruction_loss: 4.3712 - kl_loss: 2.4444\n",
            "Epoch 15/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 6.8429 - reconstruction_loss: 4.3808 - kl_loss: 2.4212\n",
            "Epoch 16/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 6.6605 - reconstruction_loss: 4.2618 - kl_loss: 2.3965\n",
            "Epoch 17/2000\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 6.6988 - reconstruction_loss: 4.2517 - kl_loss: 2.3685\n",
            "Epoch 18/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 6.3520 - reconstruction_loss: 4.0307 - kl_loss: 2.3387\n",
            "Epoch 19/2000\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 6.2818 - reconstruction_loss: 3.9757 - kl_loss: 2.3067\n",
            "Epoch 20/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 6.4698 - reconstruction_loss: 4.0315 - kl_loss: 2.2763\n",
            "Epoch 21/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 6.1454 - reconstruction_loss: 3.8998 - kl_loss: 2.2437\n",
            "Epoch 22/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 6.1253 - reconstruction_loss: 3.8968 - kl_loss: 2.2134\n",
            "Epoch 23/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 6.1445 - reconstruction_loss: 3.9071 - kl_loss: 2.1828\n",
            "Epoch 24/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 6.0052 - reconstruction_loss: 3.8504 - kl_loss: 2.1532\n",
            "Epoch 25/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 5.9312 - reconstruction_loss: 3.7522 - kl_loss: 2.1256\n",
            "Epoch 26/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 5.8511 - reconstruction_loss: 3.7175 - kl_loss: 2.0974\n",
            "Epoch 27/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 5.8065 - reconstruction_loss: 3.6731 - kl_loss: 2.0745\n",
            "Epoch 28/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 5.7069 - reconstruction_loss: 3.5506 - kl_loss: 2.0518\n",
            "Epoch 29/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 5.5711 - reconstruction_loss: 3.5460 - kl_loss: 2.0297\n",
            "Epoch 30/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 5.4750 - reconstruction_loss: 3.4684 - kl_loss: 2.0061\n",
            "Epoch 31/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 5.4685 - reconstruction_loss: 3.4302 - kl_loss: 1.9837\n",
            "Epoch 32/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 5.3949 - reconstruction_loss: 3.4292 - kl_loss: 1.9649\n",
            "Epoch 33/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 5.3310 - reconstruction_loss: 3.3839 - kl_loss: 1.9473\n",
            "Epoch 34/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 5.2754 - reconstruction_loss: 3.3745 - kl_loss: 1.9291\n",
            "Epoch 35/2000\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 5.2583 - reconstruction_loss: 3.3290 - kl_loss: 1.9112\n",
            "Epoch 36/2000\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 5.2597 - reconstruction_loss: 3.3421 - kl_loss: 1.8969\n",
            "Epoch 37/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 5.1548 - reconstruction_loss: 3.2708 - kl_loss: 1.8794\n",
            "Epoch 38/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 5.1431 - reconstruction_loss: 3.2349 - kl_loss: 1.8649\n",
            "Epoch 39/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 5.0200 - reconstruction_loss: 3.1102 - kl_loss: 1.8509\n",
            "Epoch 40/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.9487 - reconstruction_loss: 3.1135 - kl_loss: 1.8385\n",
            "Epoch 41/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.8544 - reconstruction_loss: 3.0335 - kl_loss: 1.8230\n",
            "Epoch 42/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.8615 - reconstruction_loss: 3.0712 - kl_loss: 1.8123\n",
            "Epoch 43/2000\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 4.9119 - reconstruction_loss: 2.9909 - kl_loss: 1.8011\n",
            "Epoch 44/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.8061 - reconstruction_loss: 3.0292 - kl_loss: 1.7923\n",
            "Epoch 45/2000\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 4.6581 - reconstruction_loss: 2.8614 - kl_loss: 1.7790\n",
            "Epoch 46/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.6972 - reconstruction_loss: 2.9221 - kl_loss: 1.7709\n",
            "Epoch 47/2000\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 4.8171 - reconstruction_loss: 2.9215 - kl_loss: 1.7622\n",
            "Epoch 48/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.6790 - reconstruction_loss: 2.8863 - kl_loss: 1.7518\n",
            "Epoch 49/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.5991 - reconstruction_loss: 2.8557 - kl_loss: 1.7425\n",
            "Epoch 50/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.5986 - reconstruction_loss: 2.8407 - kl_loss: 1.7340\n",
            "Epoch 51/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.5892 - reconstruction_loss: 2.8750 - kl_loss: 1.7247\n",
            "Epoch 52/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.5441 - reconstruction_loss: 2.8183 - kl_loss: 1.7158\n",
            "Epoch 53/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.5145 - reconstruction_loss: 2.8065 - kl_loss: 1.7076\n",
            "Epoch 54/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.4614 - reconstruction_loss: 2.7779 - kl_loss: 1.7007\n",
            "Epoch 55/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.5536 - reconstruction_loss: 2.8313 - kl_loss: 1.6953\n",
            "Epoch 56/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.3720 - reconstruction_loss: 2.7082 - kl_loss: 1.6838\n",
            "Epoch 57/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.4394 - reconstruction_loss: 2.7359 - kl_loss: 1.6775\n",
            "Epoch 58/2000\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 4.3824 - reconstruction_loss: 2.7217 - kl_loss: 1.6698\n",
            "Epoch 59/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.4083 - reconstruction_loss: 2.7232 - kl_loss: 1.6641\n",
            "Epoch 60/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.4164 - reconstruction_loss: 2.7125 - kl_loss: 1.6584\n",
            "Epoch 61/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.3440 - reconstruction_loss: 2.7125 - kl_loss: 1.6523\n",
            "Epoch 62/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.3712 - reconstruction_loss: 2.6781 - kl_loss: 1.6475\n",
            "Epoch 63/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 4.2921 - reconstruction_loss: 2.6627 - kl_loss: 1.6419\n",
            "Epoch 64/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.2533 - reconstruction_loss: 2.6176 - kl_loss: 1.6370\n",
            "Epoch 65/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.2215 - reconstruction_loss: 2.6076 - kl_loss: 1.6311\n",
            "Epoch 66/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.2507 - reconstruction_loss: 2.6062 - kl_loss: 1.6265\n",
            "Epoch 67/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.2065 - reconstruction_loss: 2.5626 - kl_loss: 1.6215\n",
            "Epoch 68/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 4.1974 - reconstruction_loss: 2.5577 - kl_loss: 1.6166\n",
            "Epoch 69/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 4.1826 - reconstruction_loss: 2.5587 - kl_loss: 1.6124\n",
            "Epoch 70/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.1499 - reconstruction_loss: 2.5558 - kl_loss: 1.6078\n",
            "Epoch 71/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.1998 - reconstruction_loss: 2.5867 - kl_loss: 1.6047\n",
            "Epoch 72/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.1275 - reconstruction_loss: 2.5262 - kl_loss: 1.6013\n",
            "Epoch 73/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.1086 - reconstruction_loss: 2.5224 - kl_loss: 1.5959\n",
            "Epoch 74/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.1270 - reconstruction_loss: 2.5174 - kl_loss: 1.5930\n",
            "Epoch 75/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.1068 - reconstruction_loss: 2.5174 - kl_loss: 1.5897\n",
            "Epoch 76/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.0800 - reconstruction_loss: 2.4955 - kl_loss: 1.5857\n",
            "Epoch 77/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.0878 - reconstruction_loss: 2.4977 - kl_loss: 1.5818\n",
            "Epoch 78/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.0647 - reconstruction_loss: 2.5015 - kl_loss: 1.5794\n",
            "Epoch 79/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.1308 - reconstruction_loss: 2.5043 - kl_loss: 1.5765\n",
            "Epoch 80/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.0354 - reconstruction_loss: 2.4761 - kl_loss: 1.5730\n",
            "Epoch 81/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.0870 - reconstruction_loss: 2.4780 - kl_loss: 1.5695\n",
            "Epoch 82/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.0400 - reconstruction_loss: 2.4702 - kl_loss: 1.5665\n",
            "Epoch 83/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.0500 - reconstruction_loss: 2.4690 - kl_loss: 1.5640\n",
            "Epoch 84/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.9799 - reconstruction_loss: 2.4566 - kl_loss: 1.5609\n",
            "Epoch 85/2000\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 3.9956 - reconstruction_loss: 2.4523 - kl_loss: 1.5586\n",
            "Epoch 86/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.0400 - reconstruction_loss: 2.4413 - kl_loss: 1.5560\n",
            "Epoch 87/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.9671 - reconstruction_loss: 2.4359 - kl_loss: 1.5524\n",
            "Epoch 88/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.0416 - reconstruction_loss: 2.4425 - kl_loss: 1.5512\n",
            "Epoch 89/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 4.0029 - reconstruction_loss: 2.4288 - kl_loss: 1.5483\n",
            "Epoch 90/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.9664 - reconstruction_loss: 2.4072 - kl_loss: 1.5461\n",
            "Epoch 91/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.9517 - reconstruction_loss: 2.4126 - kl_loss: 1.5434\n",
            "Epoch 92/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.9666 - reconstruction_loss: 2.4073 - kl_loss: 1.5416\n",
            "Epoch 93/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.9623 - reconstruction_loss: 2.4027 - kl_loss: 1.5390\n",
            "Epoch 94/2000\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 3.9149 - reconstruction_loss: 2.3979 - kl_loss: 1.5366\n",
            "Epoch 95/2000\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 3.9309 - reconstruction_loss: 2.3895 - kl_loss: 1.5356\n",
            "Epoch 96/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.9288 - reconstruction_loss: 2.3824 - kl_loss: 1.5330\n",
            "Epoch 97/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.8943 - reconstruction_loss: 2.3933 - kl_loss: 1.5311\n",
            "Epoch 98/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.8788 - reconstruction_loss: 2.3528 - kl_loss: 1.5286\n",
            "Epoch 99/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.9145 - reconstruction_loss: 2.3941 - kl_loss: 1.5273\n",
            "Epoch 100/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.9614 - reconstruction_loss: 2.3673 - kl_loss: 1.5260\n",
            "Epoch 101/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.8807 - reconstruction_loss: 2.3446 - kl_loss: 1.5230\n",
            "Epoch 102/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.8675 - reconstruction_loss: 2.3598 - kl_loss: 1.5208\n",
            "Epoch 103/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.8630 - reconstruction_loss: 2.3425 - kl_loss: 1.5196\n",
            "Epoch 104/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.8351 - reconstruction_loss: 2.3278 - kl_loss: 1.5170\n",
            "Epoch 105/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.8636 - reconstruction_loss: 2.3326 - kl_loss: 1.5157\n",
            "Epoch 106/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.8354 - reconstruction_loss: 2.3090 - kl_loss: 1.5133\n",
            "Epoch 107/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.8403 - reconstruction_loss: 2.3250 - kl_loss: 1.5133\n",
            "Epoch 108/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.8193 - reconstruction_loss: 2.3133 - kl_loss: 1.5106\n",
            "Epoch 109/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.7996 - reconstruction_loss: 2.3019 - kl_loss: 1.5093\n",
            "Epoch 110/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.8375 - reconstruction_loss: 2.3146 - kl_loss: 1.5080\n",
            "Epoch 111/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.7894 - reconstruction_loss: 2.2867 - kl_loss: 1.5055\n",
            "Epoch 112/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.7952 - reconstruction_loss: 2.2990 - kl_loss: 1.5043\n",
            "Epoch 113/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.8151 - reconstruction_loss: 2.3193 - kl_loss: 1.5034\n",
            "Epoch 114/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.7527 - reconstruction_loss: 2.2569 - kl_loss: 1.5007\n",
            "Epoch 115/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.8021 - reconstruction_loss: 2.2854 - kl_loss: 1.4996\n",
            "Epoch 116/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.7724 - reconstruction_loss: 2.2714 - kl_loss: 1.4977\n",
            "Epoch 117/2000\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 3.7885 - reconstruction_loss: 2.2881 - kl_loss: 1.4970\n",
            "Epoch 118/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.7960 - reconstruction_loss: 2.2771 - kl_loss: 1.4964\n",
            "Epoch 119/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.7644 - reconstruction_loss: 2.2644 - kl_loss: 1.4936\n",
            "Epoch 120/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.7799 - reconstruction_loss: 2.2820 - kl_loss: 1.4937\n",
            "Epoch 121/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.7462 - reconstruction_loss: 2.2469 - kl_loss: 1.4916\n",
            "Epoch 122/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.7536 - reconstruction_loss: 2.2606 - kl_loss: 1.4897\n",
            "Epoch 123/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.7276 - reconstruction_loss: 2.2543 - kl_loss: 1.4880\n",
            "Epoch 124/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.7403 - reconstruction_loss: 2.2557 - kl_loss: 1.4879\n",
            "Epoch 125/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.7310 - reconstruction_loss: 2.2500 - kl_loss: 1.4854\n",
            "Epoch 126/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.7243 - reconstruction_loss: 2.2399 - kl_loss: 1.4848\n",
            "Epoch 127/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.7198 - reconstruction_loss: 2.2448 - kl_loss: 1.4834\n",
            "Epoch 128/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.7426 - reconstruction_loss: 2.2406 - kl_loss: 1.4834\n",
            "Epoch 129/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.7039 - reconstruction_loss: 2.2403 - kl_loss: 1.4811\n",
            "Epoch 130/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.7084 - reconstruction_loss: 2.2251 - kl_loss: 1.4804\n",
            "Epoch 131/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.7245 - reconstruction_loss: 2.2401 - kl_loss: 1.4793\n",
            "Epoch 132/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.7087 - reconstruction_loss: 2.2338 - kl_loss: 1.4788\n",
            "Epoch 133/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6889 - reconstruction_loss: 2.2237 - kl_loss: 1.4769\n",
            "Epoch 134/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.7015 - reconstruction_loss: 2.2397 - kl_loss: 1.4756\n",
            "Epoch 135/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.7022 - reconstruction_loss: 2.2149 - kl_loss: 1.4749\n",
            "Epoch 136/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6876 - reconstruction_loss: 2.2262 - kl_loss: 1.4742\n",
            "Epoch 137/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6788 - reconstruction_loss: 2.1988 - kl_loss: 1.4732\n",
            "Epoch 138/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6918 - reconstruction_loss: 2.2118 - kl_loss: 1.4723\n",
            "Epoch 139/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6806 - reconstruction_loss: 2.2209 - kl_loss: 1.4719\n",
            "Epoch 140/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6917 - reconstruction_loss: 2.2058 - kl_loss: 1.4704\n",
            "Epoch 141/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6548 - reconstruction_loss: 2.1914 - kl_loss: 1.4695\n",
            "Epoch 142/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6623 - reconstruction_loss: 2.2016 - kl_loss: 1.4688\n",
            "Epoch 143/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6660 - reconstruction_loss: 2.1940 - kl_loss: 1.4675\n",
            "Epoch 144/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6590 - reconstruction_loss: 2.1933 - kl_loss: 1.4663\n",
            "Epoch 145/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6539 - reconstruction_loss: 2.1876 - kl_loss: 1.4657\n",
            "Epoch 146/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6648 - reconstruction_loss: 2.1943 - kl_loss: 1.4662\n",
            "Epoch 147/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6570 - reconstruction_loss: 2.1807 - kl_loss: 1.4644\n",
            "Epoch 148/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6549 - reconstruction_loss: 2.1853 - kl_loss: 1.4641\n",
            "Epoch 149/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6434 - reconstruction_loss: 2.1872 - kl_loss: 1.4625\n",
            "Epoch 150/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6530 - reconstruction_loss: 2.1856 - kl_loss: 1.4625\n",
            "Epoch 151/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6480 - reconstruction_loss: 2.1776 - kl_loss: 1.4616\n",
            "Epoch 152/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6199 - reconstruction_loss: 2.1667 - kl_loss: 1.4611\n",
            "Epoch 153/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6443 - reconstruction_loss: 2.1741 - kl_loss: 1.4600\n",
            "Epoch 154/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6337 - reconstruction_loss: 2.1630 - kl_loss: 1.4597\n",
            "Epoch 155/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6547 - reconstruction_loss: 2.1819 - kl_loss: 1.4589\n",
            "Epoch 156/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6156 - reconstruction_loss: 2.1591 - kl_loss: 1.4580\n",
            "Epoch 157/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6283 - reconstruction_loss: 2.1666 - kl_loss: 1.4573\n",
            "Epoch 158/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6035 - reconstruction_loss: 2.1545 - kl_loss: 1.4570\n",
            "Epoch 159/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6069 - reconstruction_loss: 2.1554 - kl_loss: 1.4561\n",
            "Epoch 160/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6065 - reconstruction_loss: 2.1601 - kl_loss: 1.4558\n",
            "Epoch 161/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5992 - reconstruction_loss: 2.1498 - kl_loss: 1.4548\n",
            "Epoch 162/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6028 - reconstruction_loss: 2.1490 - kl_loss: 1.4548\n",
            "Epoch 163/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6106 - reconstruction_loss: 2.1528 - kl_loss: 1.4544\n",
            "Epoch 164/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5863 - reconstruction_loss: 2.1441 - kl_loss: 1.4533\n",
            "Epoch 165/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5940 - reconstruction_loss: 2.1382 - kl_loss: 1.4526\n",
            "Epoch 166/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5911 - reconstruction_loss: 2.1366 - kl_loss: 1.4520\n",
            "Epoch 167/2000\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 3.5778 - reconstruction_loss: 2.1387 - kl_loss: 1.4515\n",
            "Epoch 168/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5918 - reconstruction_loss: 2.1384 - kl_loss: 1.4514\n",
            "Epoch 169/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5815 - reconstruction_loss: 2.1363 - kl_loss: 1.4504\n",
            "Epoch 170/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5683 - reconstruction_loss: 2.1192 - kl_loss: 1.4502\n",
            "Epoch 171/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.6039 - reconstruction_loss: 2.1363 - kl_loss: 1.4499\n",
            "Epoch 172/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5649 - reconstruction_loss: 2.1154 - kl_loss: 1.4490\n",
            "Epoch 173/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5850 - reconstruction_loss: 2.1317 - kl_loss: 1.4488\n",
            "Epoch 174/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5754 - reconstruction_loss: 2.1211 - kl_loss: 1.4481\n",
            "Epoch 175/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5575 - reconstruction_loss: 2.1125 - kl_loss: 1.4474\n",
            "Epoch 176/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5646 - reconstruction_loss: 2.1089 - kl_loss: 1.4474\n",
            "Epoch 177/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5427 - reconstruction_loss: 2.1017 - kl_loss: 1.4465\n",
            "Epoch 178/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5463 - reconstruction_loss: 2.1043 - kl_loss: 1.4464\n",
            "Epoch 179/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5548 - reconstruction_loss: 2.1067 - kl_loss: 1.4450\n",
            "Epoch 180/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5486 - reconstruction_loss: 2.1053 - kl_loss: 1.4454\n",
            "Epoch 181/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5518 - reconstruction_loss: 2.0985 - kl_loss: 1.4448\n",
            "Epoch 182/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5298 - reconstruction_loss: 2.0905 - kl_loss: 1.4442\n",
            "Epoch 183/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5524 - reconstruction_loss: 2.1080 - kl_loss: 1.4438\n",
            "Epoch 184/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5119 - reconstruction_loss: 2.0909 - kl_loss: 1.4427\n",
            "Epoch 185/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5232 - reconstruction_loss: 2.0881 - kl_loss: 1.4424\n",
            "Epoch 186/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5227 - reconstruction_loss: 2.0779 - kl_loss: 1.4425\n",
            "Epoch 187/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5156 - reconstruction_loss: 2.0813 - kl_loss: 1.4422\n",
            "Epoch 188/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5111 - reconstruction_loss: 2.0794 - kl_loss: 1.4410\n",
            "Epoch 189/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5250 - reconstruction_loss: 2.0813 - kl_loss: 1.4411\n",
            "Epoch 190/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5166 - reconstruction_loss: 2.0774 - kl_loss: 1.4408\n",
            "Epoch 191/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5082 - reconstruction_loss: 2.0676 - kl_loss: 1.4405\n",
            "Epoch 192/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5032 - reconstruction_loss: 2.0692 - kl_loss: 1.4394\n",
            "Epoch 193/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5134 - reconstruction_loss: 2.0768 - kl_loss: 1.4396\n",
            "Epoch 194/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5017 - reconstruction_loss: 2.0697 - kl_loss: 1.4389\n",
            "Epoch 195/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5047 - reconstruction_loss: 2.0616 - kl_loss: 1.4383\n",
            "Epoch 196/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.5296 - reconstruction_loss: 2.0709 - kl_loss: 1.4382\n",
            "Epoch 197/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4932 - reconstruction_loss: 2.0662 - kl_loss: 1.4373\n",
            "Epoch 198/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4903 - reconstruction_loss: 2.0581 - kl_loss: 1.4370\n",
            "Epoch 199/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4863 - reconstruction_loss: 2.0543 - kl_loss: 1.4364\n",
            "Epoch 200/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4972 - reconstruction_loss: 2.0606 - kl_loss: 1.4366\n",
            "Epoch 201/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4908 - reconstruction_loss: 2.0613 - kl_loss: 1.4354\n",
            "Epoch 202/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4704 - reconstruction_loss: 2.0474 - kl_loss: 1.4351\n",
            "Epoch 203/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4855 - reconstruction_loss: 2.0531 - kl_loss: 1.4349\n",
            "Epoch 204/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4928 - reconstruction_loss: 2.0587 - kl_loss: 1.4346\n",
            "Epoch 205/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4928 - reconstruction_loss: 2.0492 - kl_loss: 1.4343\n",
            "Epoch 206/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4797 - reconstruction_loss: 2.0504 - kl_loss: 1.4338\n",
            "Epoch 207/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4765 - reconstruction_loss: 2.0417 - kl_loss: 1.4333\n",
            "Epoch 208/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4813 - reconstruction_loss: 2.0468 - kl_loss: 1.4331\n",
            "Epoch 209/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4616 - reconstruction_loss: 2.0361 - kl_loss: 1.4334\n",
            "Epoch 210/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4715 - reconstruction_loss: 2.0469 - kl_loss: 1.4324\n",
            "Epoch 211/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4725 - reconstruction_loss: 2.0445 - kl_loss: 1.4321\n",
            "Epoch 212/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4726 - reconstruction_loss: 2.0418 - kl_loss: 1.4322\n",
            "Epoch 213/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4847 - reconstruction_loss: 2.0349 - kl_loss: 1.4316\n",
            "Epoch 214/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4793 - reconstruction_loss: 2.0461 - kl_loss: 1.4311\n",
            "Epoch 215/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4537 - reconstruction_loss: 2.0294 - kl_loss: 1.4307\n",
            "Epoch 216/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4581 - reconstruction_loss: 2.0362 - kl_loss: 1.4302\n",
            "Epoch 217/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4531 - reconstruction_loss: 2.0249 - kl_loss: 1.4302\n",
            "Epoch 218/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4599 - reconstruction_loss: 2.0266 - kl_loss: 1.4301\n",
            "Epoch 219/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4576 - reconstruction_loss: 2.0327 - kl_loss: 1.4295\n",
            "Epoch 220/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4526 - reconstruction_loss: 2.0289 - kl_loss: 1.4298\n",
            "Epoch 221/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4553 - reconstruction_loss: 2.0334 - kl_loss: 1.4288\n",
            "Epoch 222/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4428 - reconstruction_loss: 2.0274 - kl_loss: 1.4287\n",
            "Epoch 223/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4567 - reconstruction_loss: 2.0185 - kl_loss: 1.4283\n",
            "Epoch 224/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4458 - reconstruction_loss: 2.0229 - kl_loss: 1.4276\n",
            "Epoch 225/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4519 - reconstruction_loss: 2.0252 - kl_loss: 1.4279\n",
            "Epoch 226/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4444 - reconstruction_loss: 2.0221 - kl_loss: 1.4279\n",
            "Epoch 227/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4453 - reconstruction_loss: 2.0156 - kl_loss: 1.4270\n",
            "Epoch 228/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4423 - reconstruction_loss: 2.0188 - kl_loss: 1.4268\n",
            "Epoch 229/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4426 - reconstruction_loss: 2.0180 - kl_loss: 1.4272\n",
            "Epoch 230/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4367 - reconstruction_loss: 2.0145 - kl_loss: 1.4262\n",
            "Epoch 231/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4482 - reconstruction_loss: 2.0200 - kl_loss: 1.4257\n",
            "Epoch 232/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4325 - reconstruction_loss: 2.0060 - kl_loss: 1.4257\n",
            "Epoch 233/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4337 - reconstruction_loss: 2.0164 - kl_loss: 1.4255\n",
            "Epoch 234/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4401 - reconstruction_loss: 2.0123 - kl_loss: 1.4249\n",
            "Epoch 235/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4335 - reconstruction_loss: 2.0064 - kl_loss: 1.4248\n",
            "Epoch 236/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4264 - reconstruction_loss: 2.0081 - kl_loss: 1.4243\n",
            "Epoch 237/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4280 - reconstruction_loss: 2.0060 - kl_loss: 1.4238\n",
            "Epoch 238/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4525 - reconstruction_loss: 2.0119 - kl_loss: 1.4241\n",
            "Epoch 239/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4207 - reconstruction_loss: 1.9997 - kl_loss: 1.4240\n",
            "Epoch 240/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4411 - reconstruction_loss: 2.0147 - kl_loss: 1.4237\n",
            "Epoch 241/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4139 - reconstruction_loss: 2.0002 - kl_loss: 1.4232\n",
            "Epoch 242/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4229 - reconstruction_loss: 1.9995 - kl_loss: 1.4229\n",
            "Epoch 243/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4333 - reconstruction_loss: 2.0062 - kl_loss: 1.4230\n",
            "Epoch 244/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4366 - reconstruction_loss: 2.0054 - kl_loss: 1.4224\n",
            "Epoch 245/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4193 - reconstruction_loss: 2.0015 - kl_loss: 1.4223\n",
            "Epoch 246/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4156 - reconstruction_loss: 1.9975 - kl_loss: 1.4221\n",
            "Epoch 247/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4203 - reconstruction_loss: 1.9979 - kl_loss: 1.4216\n",
            "Epoch 248/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4065 - reconstruction_loss: 1.9929 - kl_loss: 1.4212\n",
            "Epoch 249/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4164 - reconstruction_loss: 1.9892 - kl_loss: 1.4218\n",
            "Epoch 250/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4245 - reconstruction_loss: 1.9996 - kl_loss: 1.4214\n",
            "Epoch 251/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4040 - reconstruction_loss: 1.9872 - kl_loss: 1.4202\n",
            "Epoch 252/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4082 - reconstruction_loss: 1.9947 - kl_loss: 1.4205\n",
            "Epoch 253/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4106 - reconstruction_loss: 1.9859 - kl_loss: 1.4208\n",
            "Epoch 254/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4034 - reconstruction_loss: 1.9907 - kl_loss: 1.4201\n",
            "Epoch 255/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3993 - reconstruction_loss: 1.9793 - kl_loss: 1.4201\n",
            "Epoch 256/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4084 - reconstruction_loss: 1.9880 - kl_loss: 1.4197\n",
            "Epoch 257/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4020 - reconstruction_loss: 1.9874 - kl_loss: 1.4197\n",
            "Epoch 258/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.4058 - reconstruction_loss: 1.9850 - kl_loss: 1.4191\n",
            "Epoch 259/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3948 - reconstruction_loss: 1.9820 - kl_loss: 1.4202\n",
            "Epoch 260/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3932 - reconstruction_loss: 1.9757 - kl_loss: 1.4183\n",
            "Epoch 261/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3949 - reconstruction_loss: 1.9820 - kl_loss: 1.4186\n",
            "Epoch 262/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3923 - reconstruction_loss: 1.9795 - kl_loss: 1.4184\n",
            "Epoch 263/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3912 - reconstruction_loss: 1.9764 - kl_loss: 1.4177\n",
            "Epoch 264/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3955 - reconstruction_loss: 1.9825 - kl_loss: 1.4177\n",
            "Epoch 265/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3970 - reconstruction_loss: 1.9747 - kl_loss: 1.4176\n",
            "Epoch 266/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3768 - reconstruction_loss: 1.9741 - kl_loss: 1.4175\n",
            "Epoch 267/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3916 - reconstruction_loss: 1.9726 - kl_loss: 1.4177\n",
            "Epoch 268/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3898 - reconstruction_loss: 1.9787 - kl_loss: 1.4171\n",
            "Epoch 269/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3947 - reconstruction_loss: 1.9743 - kl_loss: 1.4166\n",
            "Epoch 270/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3797 - reconstruction_loss: 1.9653 - kl_loss: 1.4165\n",
            "Epoch 271/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3865 - reconstruction_loss: 1.9736 - kl_loss: 1.4162\n",
            "Epoch 272/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3870 - reconstruction_loss: 1.9731 - kl_loss: 1.4161\n",
            "Epoch 273/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3907 - reconstruction_loss: 1.9707 - kl_loss: 1.4156\n",
            "Epoch 274/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3894 - reconstruction_loss: 1.9767 - kl_loss: 1.4157\n",
            "Epoch 275/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3755 - reconstruction_loss: 1.9658 - kl_loss: 1.4157\n",
            "Epoch 276/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3721 - reconstruction_loss: 1.9618 - kl_loss: 1.4152\n",
            "Epoch 277/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3710 - reconstruction_loss: 1.9608 - kl_loss: 1.4154\n",
            "Epoch 278/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3724 - reconstruction_loss: 1.9598 - kl_loss: 1.4153\n",
            "Epoch 279/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3728 - reconstruction_loss: 1.9581 - kl_loss: 1.4149\n",
            "Epoch 280/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3705 - reconstruction_loss: 1.9606 - kl_loss: 1.4148\n",
            "Epoch 281/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3723 - reconstruction_loss: 1.9561 - kl_loss: 1.4143\n",
            "Epoch 282/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3747 - reconstruction_loss: 1.9573 - kl_loss: 1.4146\n",
            "Epoch 283/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3661 - reconstruction_loss: 1.9532 - kl_loss: 1.4141\n",
            "Epoch 284/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3632 - reconstruction_loss: 1.9529 - kl_loss: 1.4141\n",
            "Epoch 285/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3751 - reconstruction_loss: 1.9548 - kl_loss: 1.4135\n",
            "Epoch 286/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3659 - reconstruction_loss: 1.9567 - kl_loss: 1.4133\n",
            "Epoch 287/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3641 - reconstruction_loss: 1.9570 - kl_loss: 1.4135\n",
            "Epoch 288/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3575 - reconstruction_loss: 1.9446 - kl_loss: 1.4133\n",
            "Epoch 289/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3717 - reconstruction_loss: 1.9560 - kl_loss: 1.4129\n",
            "Epoch 290/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3601 - reconstruction_loss: 1.9516 - kl_loss: 1.4128\n",
            "Epoch 291/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3531 - reconstruction_loss: 1.9497 - kl_loss: 1.4127\n",
            "Epoch 292/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3598 - reconstruction_loss: 1.9534 - kl_loss: 1.4126\n",
            "Epoch 293/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3551 - reconstruction_loss: 1.9475 - kl_loss: 1.4124\n",
            "Epoch 294/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3522 - reconstruction_loss: 1.9478 - kl_loss: 1.4121\n",
            "Epoch 295/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3485 - reconstruction_loss: 1.9465 - kl_loss: 1.4120\n",
            "Epoch 296/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3586 - reconstruction_loss: 1.9428 - kl_loss: 1.4120\n",
            "Epoch 297/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3451 - reconstruction_loss: 1.9415 - kl_loss: 1.4114\n",
            "Epoch 298/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3513 - reconstruction_loss: 1.9466 - kl_loss: 1.4115\n",
            "Epoch 299/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3642 - reconstruction_loss: 1.9495 - kl_loss: 1.4117\n",
            "Epoch 300/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3432 - reconstruction_loss: 1.9379 - kl_loss: 1.4104\n",
            "Epoch 301/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3552 - reconstruction_loss: 1.9460 - kl_loss: 1.4113\n",
            "Epoch 302/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3493 - reconstruction_loss: 1.9438 - kl_loss: 1.4104\n",
            "Epoch 303/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3484 - reconstruction_loss: 1.9385 - kl_loss: 1.4107\n",
            "Epoch 304/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3506 - reconstruction_loss: 1.9402 - kl_loss: 1.4107\n",
            "Epoch 305/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3514 - reconstruction_loss: 1.9420 - kl_loss: 1.4101\n",
            "Epoch 306/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3436 - reconstruction_loss: 1.9383 - kl_loss: 1.4104\n",
            "Epoch 307/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3515 - reconstruction_loss: 1.9414 - kl_loss: 1.4098\n",
            "Epoch 308/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3431 - reconstruction_loss: 1.9330 - kl_loss: 1.4096\n",
            "Epoch 309/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3406 - reconstruction_loss: 1.9341 - kl_loss: 1.4096\n",
            "Epoch 310/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3528 - reconstruction_loss: 1.9428 - kl_loss: 1.4092\n",
            "Epoch 311/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3405 - reconstruction_loss: 1.9353 - kl_loss: 1.4092\n",
            "Epoch 312/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3388 - reconstruction_loss: 1.9359 - kl_loss: 1.4089\n",
            "Epoch 313/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3454 - reconstruction_loss: 1.9378 - kl_loss: 1.4094\n",
            "Epoch 314/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3358 - reconstruction_loss: 1.9300 - kl_loss: 1.4092\n",
            "Epoch 315/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3330 - reconstruction_loss: 1.9310 - kl_loss: 1.4084\n",
            "Epoch 316/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3521 - reconstruction_loss: 1.9344 - kl_loss: 1.4087\n",
            "Epoch 317/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3292 - reconstruction_loss: 1.9252 - kl_loss: 1.4087\n",
            "Epoch 318/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3336 - reconstruction_loss: 1.9264 - kl_loss: 1.4081\n",
            "Epoch 319/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3321 - reconstruction_loss: 1.9263 - kl_loss: 1.4079\n",
            "Epoch 320/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3352 - reconstruction_loss: 1.9246 - kl_loss: 1.4081\n",
            "Epoch 321/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3228 - reconstruction_loss: 1.9219 - kl_loss: 1.4078\n",
            "Epoch 322/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3329 - reconstruction_loss: 1.9227 - kl_loss: 1.4076\n",
            "Epoch 323/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3302 - reconstruction_loss: 1.9278 - kl_loss: 1.4075\n",
            "Epoch 324/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3169 - reconstruction_loss: 1.9162 - kl_loss: 1.4076\n",
            "Epoch 325/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3281 - reconstruction_loss: 1.9223 - kl_loss: 1.4071\n",
            "Epoch 326/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3216 - reconstruction_loss: 1.9158 - kl_loss: 1.4072\n",
            "Epoch 327/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3288 - reconstruction_loss: 1.9232 - kl_loss: 1.4072\n",
            "Epoch 328/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3156 - reconstruction_loss: 1.9135 - kl_loss: 1.4068\n",
            "Epoch 329/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3256 - reconstruction_loss: 1.9177 - kl_loss: 1.4064\n",
            "Epoch 330/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3229 - reconstruction_loss: 1.9190 - kl_loss: 1.4071\n",
            "Epoch 331/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3283 - reconstruction_loss: 1.9187 - kl_loss: 1.4065\n",
            "Epoch 332/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3220 - reconstruction_loss: 1.9175 - kl_loss: 1.4064\n",
            "Epoch 333/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3173 - reconstruction_loss: 1.9150 - kl_loss: 1.4062\n",
            "Epoch 334/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3208 - reconstruction_loss: 1.9134 - kl_loss: 1.4064\n",
            "Epoch 335/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3139 - reconstruction_loss: 1.9113 - kl_loss: 1.4059\n",
            "Epoch 336/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3140 - reconstruction_loss: 1.9159 - kl_loss: 1.4060\n",
            "Epoch 337/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3154 - reconstruction_loss: 1.9109 - kl_loss: 1.4056\n",
            "Epoch 338/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3089 - reconstruction_loss: 1.9106 - kl_loss: 1.4055\n",
            "Epoch 339/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3093 - reconstruction_loss: 1.9124 - kl_loss: 1.4057\n",
            "Epoch 340/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3087 - reconstruction_loss: 1.9092 - kl_loss: 1.4056\n",
            "Epoch 341/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3143 - reconstruction_loss: 1.9098 - kl_loss: 1.4052\n",
            "Epoch 342/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3135 - reconstruction_loss: 1.9184 - kl_loss: 1.4049\n",
            "Epoch 343/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2994 - reconstruction_loss: 1.9024 - kl_loss: 1.4052\n",
            "Epoch 344/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3171 - reconstruction_loss: 1.9066 - kl_loss: 1.4049\n",
            "Epoch 345/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3128 - reconstruction_loss: 1.9073 - kl_loss: 1.4048\n",
            "Epoch 346/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3158 - reconstruction_loss: 1.9077 - kl_loss: 1.4047\n",
            "Epoch 347/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3103 - reconstruction_loss: 1.9089 - kl_loss: 1.4039\n",
            "Epoch 348/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3074 - reconstruction_loss: 1.9040 - kl_loss: 1.4047\n",
            "Epoch 349/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3083 - reconstruction_loss: 1.9051 - kl_loss: 1.4041\n",
            "Epoch 350/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3046 - reconstruction_loss: 1.9051 - kl_loss: 1.4035\n",
            "Epoch 351/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3068 - reconstruction_loss: 1.9036 - kl_loss: 1.4039\n",
            "Epoch 352/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3029 - reconstruction_loss: 1.9028 - kl_loss: 1.4039\n",
            "Epoch 353/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3001 - reconstruction_loss: 1.8991 - kl_loss: 1.4037\n",
            "Epoch 354/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3088 - reconstruction_loss: 1.9049 - kl_loss: 1.4038\n",
            "Epoch 355/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3038 - reconstruction_loss: 1.9023 - kl_loss: 1.4035\n",
            "Epoch 356/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3038 - reconstruction_loss: 1.8994 - kl_loss: 1.4032\n",
            "Epoch 357/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3014 - reconstruction_loss: 1.9017 - kl_loss: 1.4033\n",
            "Epoch 358/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2959 - reconstruction_loss: 1.9022 - kl_loss: 1.4032\n",
            "Epoch 359/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3041 - reconstruction_loss: 1.8954 - kl_loss: 1.4037\n",
            "Epoch 360/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.3040 - reconstruction_loss: 1.9007 - kl_loss: 1.4031\n",
            "Epoch 361/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2995 - reconstruction_loss: 1.9009 - kl_loss: 1.4026\n",
            "Epoch 362/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2961 - reconstruction_loss: 1.9006 - kl_loss: 1.4028\n",
            "Epoch 363/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2895 - reconstruction_loss: 1.8987 - kl_loss: 1.4026\n",
            "Epoch 364/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2926 - reconstruction_loss: 1.8933 - kl_loss: 1.4026\n",
            "Epoch 365/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2919 - reconstruction_loss: 1.8934 - kl_loss: 1.4025\n",
            "Epoch 366/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2934 - reconstruction_loss: 1.8976 - kl_loss: 1.4023\n",
            "Epoch 367/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2968 - reconstruction_loss: 1.8985 - kl_loss: 1.4023\n",
            "Epoch 368/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2858 - reconstruction_loss: 1.8926 - kl_loss: 1.4022\n",
            "Epoch 369/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2854 - reconstruction_loss: 1.8924 - kl_loss: 1.4021\n",
            "Epoch 370/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2913 - reconstruction_loss: 1.8949 - kl_loss: 1.4014\n",
            "Epoch 371/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2933 - reconstruction_loss: 1.8920 - kl_loss: 1.4018\n",
            "Epoch 372/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2959 - reconstruction_loss: 1.8993 - kl_loss: 1.4018\n",
            "Epoch 373/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2963 - reconstruction_loss: 1.8934 - kl_loss: 1.4018\n",
            "Epoch 374/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2825 - reconstruction_loss: 1.8879 - kl_loss: 1.4015\n",
            "Epoch 375/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2935 - reconstruction_loss: 1.8943 - kl_loss: 1.4012\n",
            "Epoch 376/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2842 - reconstruction_loss: 1.8886 - kl_loss: 1.4014\n",
            "Epoch 377/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2871 - reconstruction_loss: 1.8890 - kl_loss: 1.4012\n",
            "Epoch 378/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2958 - reconstruction_loss: 1.8957 - kl_loss: 1.4012\n",
            "Epoch 379/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.2755 - reconstruction_loss: 1.8865 - kl_loss: 1.4012\n",
            "Epoch 380/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2832 - reconstruction_loss: 1.8856 - kl_loss: 1.4011\n",
            "Epoch 381/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2921 - reconstruction_loss: 1.8893 - kl_loss: 1.4009\n",
            "Epoch 382/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2838 - reconstruction_loss: 1.8839 - kl_loss: 1.4004\n",
            "Epoch 383/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2897 - reconstruction_loss: 1.8892 - kl_loss: 1.4006\n",
            "Epoch 384/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2849 - reconstruction_loss: 1.8902 - kl_loss: 1.4008\n",
            "Epoch 385/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2820 - reconstruction_loss: 1.8850 - kl_loss: 1.4004\n",
            "Epoch 386/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2776 - reconstruction_loss: 1.8855 - kl_loss: 1.4007\n",
            "Epoch 387/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2849 - reconstruction_loss: 1.8881 - kl_loss: 1.4003\n",
            "Epoch 388/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2820 - reconstruction_loss: 1.8837 - kl_loss: 1.4002\n",
            "Epoch 389/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2822 - reconstruction_loss: 1.8873 - kl_loss: 1.4002\n",
            "Epoch 390/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2859 - reconstruction_loss: 1.8901 - kl_loss: 1.3999\n",
            "Epoch 391/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2800 - reconstruction_loss: 1.8806 - kl_loss: 1.3999\n",
            "Epoch 392/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2777 - reconstruction_loss: 1.8823 - kl_loss: 1.3997\n",
            "Epoch 393/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2831 - reconstruction_loss: 1.8843 - kl_loss: 1.3995\n",
            "Epoch 394/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2707 - reconstruction_loss: 1.8797 - kl_loss: 1.3994\n",
            "Epoch 395/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2808 - reconstruction_loss: 1.8812 - kl_loss: 1.4000\n",
            "Epoch 396/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2713 - reconstruction_loss: 1.8822 - kl_loss: 1.3994\n",
            "Epoch 397/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2824 - reconstruction_loss: 1.8798 - kl_loss: 1.3994\n",
            "Epoch 398/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2739 - reconstruction_loss: 1.8806 - kl_loss: 1.3992\n",
            "Epoch 399/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2744 - reconstruction_loss: 1.8795 - kl_loss: 1.3992\n",
            "Epoch 400/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2731 - reconstruction_loss: 1.8806 - kl_loss: 1.3993\n",
            "Epoch 401/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2755 - reconstruction_loss: 1.8783 - kl_loss: 1.3992\n",
            "Epoch 402/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2787 - reconstruction_loss: 1.8813 - kl_loss: 1.3987\n",
            "Epoch 403/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2739 - reconstruction_loss: 1.8745 - kl_loss: 1.3991\n",
            "Epoch 404/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2786 - reconstruction_loss: 1.8831 - kl_loss: 1.3984\n",
            "Epoch 405/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2744 - reconstruction_loss: 1.8778 - kl_loss: 1.3986\n",
            "Epoch 406/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2664 - reconstruction_loss: 1.8771 - kl_loss: 1.3984\n",
            "Epoch 407/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2655 - reconstruction_loss: 1.8722 - kl_loss: 1.3982\n",
            "Epoch 408/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2651 - reconstruction_loss: 1.8759 - kl_loss: 1.3985\n",
            "Epoch 409/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2616 - reconstruction_loss: 1.8728 - kl_loss: 1.3983\n",
            "Epoch 410/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2678 - reconstruction_loss: 1.8722 - kl_loss: 1.3985\n",
            "Epoch 411/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2665 - reconstruction_loss: 1.8676 - kl_loss: 1.3984\n",
            "Epoch 412/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2715 - reconstruction_loss: 1.8716 - kl_loss: 1.3981\n",
            "Epoch 413/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.2660 - reconstruction_loss: 1.8742 - kl_loss: 1.3981\n",
            "Epoch 414/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2606 - reconstruction_loss: 1.8716 - kl_loss: 1.3984\n",
            "Epoch 415/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2645 - reconstruction_loss: 1.8721 - kl_loss: 1.3981\n",
            "Epoch 416/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2627 - reconstruction_loss: 1.8696 - kl_loss: 1.3979\n",
            "Epoch 417/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2633 - reconstruction_loss: 1.8735 - kl_loss: 1.3980\n",
            "Epoch 418/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2666 - reconstruction_loss: 1.8660 - kl_loss: 1.3978\n",
            "Epoch 419/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2636 - reconstruction_loss: 1.8684 - kl_loss: 1.3976\n",
            "Epoch 420/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2622 - reconstruction_loss: 1.8669 - kl_loss: 1.3975\n",
            "Epoch 421/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2718 - reconstruction_loss: 1.8680 - kl_loss: 1.3975\n",
            "Epoch 422/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2619 - reconstruction_loss: 1.8659 - kl_loss: 1.3975\n",
            "Epoch 423/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2609 - reconstruction_loss: 1.8664 - kl_loss: 1.3972\n",
            "Epoch 424/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2536 - reconstruction_loss: 1.8638 - kl_loss: 1.3970\n",
            "Epoch 425/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2597 - reconstruction_loss: 1.8655 - kl_loss: 1.3973\n",
            "Epoch 426/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2671 - reconstruction_loss: 1.8683 - kl_loss: 1.3972\n",
            "Epoch 427/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2604 - reconstruction_loss: 1.8666 - kl_loss: 1.3973\n",
            "Epoch 428/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2527 - reconstruction_loss: 1.8620 - kl_loss: 1.3968\n",
            "Epoch 429/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2577 - reconstruction_loss: 1.8631 - kl_loss: 1.3969\n",
            "Epoch 430/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2574 - reconstruction_loss: 1.8633 - kl_loss: 1.3967\n",
            "Epoch 431/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2625 - reconstruction_loss: 1.8677 - kl_loss: 1.3967\n",
            "Epoch 432/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2565 - reconstruction_loss: 1.8614 - kl_loss: 1.3967\n",
            "Epoch 433/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2572 - reconstruction_loss: 1.8630 - kl_loss: 1.3969\n",
            "Epoch 434/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2460 - reconstruction_loss: 1.8621 - kl_loss: 1.3963\n",
            "Epoch 435/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2507 - reconstruction_loss: 1.8599 - kl_loss: 1.3967\n",
            "Epoch 436/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2526 - reconstruction_loss: 1.8639 - kl_loss: 1.3962\n",
            "Epoch 437/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2518 - reconstruction_loss: 1.8624 - kl_loss: 1.3966\n",
            "Epoch 438/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2491 - reconstruction_loss: 1.8581 - kl_loss: 1.3961\n",
            "Epoch 439/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2520 - reconstruction_loss: 1.8605 - kl_loss: 1.3961\n",
            "Epoch 440/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2538 - reconstruction_loss: 1.8602 - kl_loss: 1.3960\n",
            "Epoch 441/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2482 - reconstruction_loss: 1.8605 - kl_loss: 1.3960\n",
            "Epoch 442/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2508 - reconstruction_loss: 1.8637 - kl_loss: 1.3961\n",
            "Epoch 443/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2487 - reconstruction_loss: 1.8574 - kl_loss: 1.3957\n",
            "Epoch 444/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2425 - reconstruction_loss: 1.8566 - kl_loss: 1.3959\n",
            "Epoch 445/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2572 - reconstruction_loss: 1.8572 - kl_loss: 1.3956\n",
            "Epoch 446/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2481 - reconstruction_loss: 1.8554 - kl_loss: 1.3959\n",
            "Epoch 447/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2527 - reconstruction_loss: 1.8595 - kl_loss: 1.3957\n",
            "Epoch 448/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2458 - reconstruction_loss: 1.8560 - kl_loss: 1.3954\n",
            "Epoch 449/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2436 - reconstruction_loss: 1.8580 - kl_loss: 1.3961\n",
            "Epoch 450/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2524 - reconstruction_loss: 1.8551 - kl_loss: 1.3957\n",
            "Epoch 451/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2460 - reconstruction_loss: 1.8557 - kl_loss: 1.3951\n",
            "Epoch 452/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2477 - reconstruction_loss: 1.8568 - kl_loss: 1.3957\n",
            "Epoch 453/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2472 - reconstruction_loss: 1.8520 - kl_loss: 1.3952\n",
            "Epoch 454/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.2471 - reconstruction_loss: 1.8566 - kl_loss: 1.3956\n",
            "Epoch 455/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2481 - reconstruction_loss: 1.8558 - kl_loss: 1.3951\n",
            "Epoch 456/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2394 - reconstruction_loss: 1.8491 - kl_loss: 1.3954\n",
            "Epoch 457/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2481 - reconstruction_loss: 1.8558 - kl_loss: 1.3952\n",
            "Epoch 458/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2486 - reconstruction_loss: 1.8566 - kl_loss: 1.3955\n",
            "Epoch 459/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2415 - reconstruction_loss: 1.8567 - kl_loss: 1.3948\n",
            "Epoch 460/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2415 - reconstruction_loss: 1.8515 - kl_loss: 1.3950\n",
            "Epoch 461/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2363 - reconstruction_loss: 1.8505 - kl_loss: 1.3947\n",
            "Epoch 462/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2385 - reconstruction_loss: 1.8495 - kl_loss: 1.3948\n",
            "Epoch 463/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2421 - reconstruction_loss: 1.8531 - kl_loss: 1.3944\n",
            "Epoch 464/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2425 - reconstruction_loss: 1.8516 - kl_loss: 1.3948\n",
            "Epoch 465/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2461 - reconstruction_loss: 1.8519 - kl_loss: 1.3943\n",
            "Epoch 466/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2418 - reconstruction_loss: 1.8505 - kl_loss: 1.3944\n",
            "Epoch 467/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2404 - reconstruction_loss: 1.8495 - kl_loss: 1.3943\n",
            "Epoch 468/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2383 - reconstruction_loss: 1.8468 - kl_loss: 1.3944\n",
            "Epoch 469/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2431 - reconstruction_loss: 1.8485 - kl_loss: 1.3942\n",
            "Epoch 470/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2321 - reconstruction_loss: 1.8485 - kl_loss: 1.3944\n",
            "Epoch 471/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2394 - reconstruction_loss: 1.8484 - kl_loss: 1.3939\n",
            "Epoch 472/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2343 - reconstruction_loss: 1.8453 - kl_loss: 1.3938\n",
            "Epoch 473/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2405 - reconstruction_loss: 1.8463 - kl_loss: 1.3939\n",
            "Epoch 474/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2357 - reconstruction_loss: 1.8457 - kl_loss: 1.3941\n",
            "Epoch 475/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2448 - reconstruction_loss: 1.8507 - kl_loss: 1.3943\n",
            "Epoch 476/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2367 - reconstruction_loss: 1.8446 - kl_loss: 1.3942\n",
            "Epoch 477/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2324 - reconstruction_loss: 1.8445 - kl_loss: 1.3938\n",
            "Epoch 478/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2375 - reconstruction_loss: 1.8453 - kl_loss: 1.3939\n",
            "Epoch 479/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2313 - reconstruction_loss: 1.8418 - kl_loss: 1.3934\n",
            "Epoch 480/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2374 - reconstruction_loss: 1.8441 - kl_loss: 1.3936\n",
            "Epoch 481/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2298 - reconstruction_loss: 1.8427 - kl_loss: 1.3938\n",
            "Epoch 482/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2302 - reconstruction_loss: 1.8417 - kl_loss: 1.3935\n",
            "Epoch 483/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2351 - reconstruction_loss: 1.8409 - kl_loss: 1.3934\n",
            "Epoch 484/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2355 - reconstruction_loss: 1.8443 - kl_loss: 1.3934\n",
            "Epoch 485/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2300 - reconstruction_loss: 1.8442 - kl_loss: 1.3934\n",
            "Epoch 486/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2284 - reconstruction_loss: 1.8381 - kl_loss: 1.3935\n",
            "Epoch 487/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2363 - reconstruction_loss: 1.8420 - kl_loss: 1.3932\n",
            "Epoch 488/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2284 - reconstruction_loss: 1.8409 - kl_loss: 1.3933\n",
            "Epoch 489/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2237 - reconstruction_loss: 1.8384 - kl_loss: 1.3929\n",
            "Epoch 490/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.2339 - reconstruction_loss: 1.8398 - kl_loss: 1.3934\n",
            "Epoch 491/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2313 - reconstruction_loss: 1.8402 - kl_loss: 1.3932\n",
            "Epoch 492/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2211 - reconstruction_loss: 1.8361 - kl_loss: 1.3927\n",
            "Epoch 493/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2305 - reconstruction_loss: 1.8399 - kl_loss: 1.3929\n",
            "Epoch 494/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2314 - reconstruction_loss: 1.8409 - kl_loss: 1.3930\n",
            "Epoch 495/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2284 - reconstruction_loss: 1.8384 - kl_loss: 1.3925\n",
            "Epoch 496/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2233 - reconstruction_loss: 1.8350 - kl_loss: 1.3931\n",
            "Epoch 497/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2285 - reconstruction_loss: 1.8398 - kl_loss: 1.3925\n",
            "Epoch 498/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2269 - reconstruction_loss: 1.8374 - kl_loss: 1.3926\n",
            "Epoch 499/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2300 - reconstruction_loss: 1.8371 - kl_loss: 1.3925\n",
            "Epoch 500/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2211 - reconstruction_loss: 1.8368 - kl_loss: 1.3926\n",
            "Epoch 501/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2249 - reconstruction_loss: 1.8352 - kl_loss: 1.3925\n",
            "Epoch 502/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2245 - reconstruction_loss: 1.8351 - kl_loss: 1.3923\n",
            "Epoch 503/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2182 - reconstruction_loss: 1.8355 - kl_loss: 1.3928\n",
            "Epoch 504/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2212 - reconstruction_loss: 1.8394 - kl_loss: 1.3924\n",
            "Epoch 505/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2269 - reconstruction_loss: 1.8363 - kl_loss: 1.3920\n",
            "Epoch 506/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2253 - reconstruction_loss: 1.8360 - kl_loss: 1.3923\n",
            "Epoch 507/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2218 - reconstruction_loss: 1.8333 - kl_loss: 1.3925\n",
            "Epoch 508/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2196 - reconstruction_loss: 1.8355 - kl_loss: 1.3922\n",
            "Epoch 509/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2185 - reconstruction_loss: 1.8336 - kl_loss: 1.3922\n",
            "Epoch 510/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2242 - reconstruction_loss: 1.8340 - kl_loss: 1.3924\n",
            "Epoch 511/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2199 - reconstruction_loss: 1.8307 - kl_loss: 1.3919\n",
            "Epoch 512/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2234 - reconstruction_loss: 1.8332 - kl_loss: 1.3919\n",
            "Epoch 513/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2165 - reconstruction_loss: 1.8345 - kl_loss: 1.3919\n",
            "Epoch 514/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2223 - reconstruction_loss: 1.8320 - kl_loss: 1.3920\n",
            "Epoch 515/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2227 - reconstruction_loss: 1.8322 - kl_loss: 1.3917\n",
            "Epoch 516/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2179 - reconstruction_loss: 1.8319 - kl_loss: 1.3919\n",
            "Epoch 517/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2172 - reconstruction_loss: 1.8299 - kl_loss: 1.3918\n",
            "Epoch 518/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2167 - reconstruction_loss: 1.8306 - kl_loss: 1.3916\n",
            "Epoch 519/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2252 - reconstruction_loss: 1.8342 - kl_loss: 1.3920\n",
            "Epoch 520/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2165 - reconstruction_loss: 1.8292 - kl_loss: 1.3917\n",
            "Epoch 521/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2247 - reconstruction_loss: 1.8334 - kl_loss: 1.3915\n",
            "Epoch 522/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2162 - reconstruction_loss: 1.8316 - kl_loss: 1.3915\n",
            "Epoch 523/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2155 - reconstruction_loss: 1.8277 - kl_loss: 1.3914\n",
            "Epoch 524/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.2185 - reconstruction_loss: 1.8304 - kl_loss: 1.3914\n",
            "Epoch 525/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2190 - reconstruction_loss: 1.8294 - kl_loss: 1.3914\n",
            "Epoch 526/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2169 - reconstruction_loss: 1.8331 - kl_loss: 1.3914\n",
            "Epoch 527/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2275 - reconstruction_loss: 1.8291 - kl_loss: 1.3915\n",
            "Epoch 528/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2135 - reconstruction_loss: 1.8287 - kl_loss: 1.3912\n",
            "Epoch 529/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2145 - reconstruction_loss: 1.8271 - kl_loss: 1.3913\n",
            "Epoch 530/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2173 - reconstruction_loss: 1.8266 - kl_loss: 1.3909\n",
            "Epoch 531/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2150 - reconstruction_loss: 1.8271 - kl_loss: 1.3912\n",
            "Epoch 532/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2185 - reconstruction_loss: 1.8303 - kl_loss: 1.3910\n",
            "Epoch 533/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2123 - reconstruction_loss: 1.8288 - kl_loss: 1.3911\n",
            "Epoch 534/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2152 - reconstruction_loss: 1.8248 - kl_loss: 1.3909\n",
            "Epoch 535/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2081 - reconstruction_loss: 1.8267 - kl_loss: 1.3911\n",
            "Epoch 536/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.2192 - reconstruction_loss: 1.8298 - kl_loss: 1.3904\n",
            "Epoch 537/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2204 - reconstruction_loss: 1.8254 - kl_loss: 1.3907\n",
            "Epoch 538/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2063 - reconstruction_loss: 1.8240 - kl_loss: 1.3906\n",
            "Epoch 539/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2152 - reconstruction_loss: 1.8278 - kl_loss: 1.3909\n",
            "Epoch 540/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2103 - reconstruction_loss: 1.8232 - kl_loss: 1.3910\n",
            "Epoch 541/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.2074 - reconstruction_loss: 1.8252 - kl_loss: 1.3903\n",
            "Epoch 542/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2090 - reconstruction_loss: 1.8249 - kl_loss: 1.3905\n",
            "Epoch 543/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2072 - reconstruction_loss: 1.8257 - kl_loss: 1.3902\n",
            "Epoch 544/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2057 - reconstruction_loss: 1.8209 - kl_loss: 1.3910\n",
            "Epoch 545/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2082 - reconstruction_loss: 1.8253 - kl_loss: 1.3904\n",
            "Epoch 546/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2059 - reconstruction_loss: 1.8230 - kl_loss: 1.3900\n",
            "Epoch 547/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2100 - reconstruction_loss: 1.8224 - kl_loss: 1.3905\n",
            "Epoch 548/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2085 - reconstruction_loss: 1.8234 - kl_loss: 1.3905\n",
            "Epoch 549/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2136 - reconstruction_loss: 1.8232 - kl_loss: 1.3909\n",
            "Epoch 550/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2089 - reconstruction_loss: 1.8233 - kl_loss: 1.3904\n",
            "Epoch 551/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2089 - reconstruction_loss: 1.8219 - kl_loss: 1.3901\n",
            "Epoch 552/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2025 - reconstruction_loss: 1.8199 - kl_loss: 1.3901\n",
            "Epoch 553/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.2017 - reconstruction_loss: 1.8207 - kl_loss: 1.3902\n",
            "Epoch 554/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2092 - reconstruction_loss: 1.8190 - kl_loss: 1.3901\n",
            "Epoch 555/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2067 - reconstruction_loss: 1.8213 - kl_loss: 1.3897\n",
            "Epoch 556/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2039 - reconstruction_loss: 1.8212 - kl_loss: 1.3898\n",
            "Epoch 557/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2065 - reconstruction_loss: 1.8203 - kl_loss: 1.3896\n",
            "Epoch 558/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2079 - reconstruction_loss: 1.8228 - kl_loss: 1.3896\n",
            "Epoch 559/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2050 - reconstruction_loss: 1.8190 - kl_loss: 1.3897\n",
            "Epoch 560/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2068 - reconstruction_loss: 1.8178 - kl_loss: 1.3898\n",
            "Epoch 561/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2033 - reconstruction_loss: 1.8204 - kl_loss: 1.3896\n",
            "Epoch 562/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2029 - reconstruction_loss: 1.8176 - kl_loss: 1.3898\n",
            "Epoch 563/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2024 - reconstruction_loss: 1.8158 - kl_loss: 1.3895\n",
            "Epoch 564/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2054 - reconstruction_loss: 1.8184 - kl_loss: 1.3897\n",
            "Epoch 565/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2029 - reconstruction_loss: 1.8156 - kl_loss: 1.3898\n",
            "Epoch 566/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2080 - reconstruction_loss: 1.8187 - kl_loss: 1.3895\n",
            "Epoch 567/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1973 - reconstruction_loss: 1.8156 - kl_loss: 1.3895\n",
            "Epoch 568/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1979 - reconstruction_loss: 1.8184 - kl_loss: 1.3894\n",
            "Epoch 569/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2006 - reconstruction_loss: 1.8158 - kl_loss: 1.3895\n",
            "Epoch 570/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2016 - reconstruction_loss: 1.8163 - kl_loss: 1.3894\n",
            "Epoch 571/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2034 - reconstruction_loss: 1.8168 - kl_loss: 1.3896\n",
            "Epoch 572/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2033 - reconstruction_loss: 1.8184 - kl_loss: 1.3894\n",
            "Epoch 573/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1997 - reconstruction_loss: 1.8135 - kl_loss: 1.3891\n",
            "Epoch 574/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1962 - reconstruction_loss: 1.8152 - kl_loss: 1.3889\n",
            "Epoch 575/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2001 - reconstruction_loss: 1.8147 - kl_loss: 1.3889\n",
            "Epoch 576/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1980 - reconstruction_loss: 1.8164 - kl_loss: 1.3889\n",
            "Epoch 577/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1962 - reconstruction_loss: 1.8129 - kl_loss: 1.3894\n",
            "Epoch 578/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1998 - reconstruction_loss: 1.8129 - kl_loss: 1.3891\n",
            "Epoch 579/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2011 - reconstruction_loss: 1.8169 - kl_loss: 1.3891\n",
            "Epoch 580/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1974 - reconstruction_loss: 1.8136 - kl_loss: 1.3894\n",
            "Epoch 581/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2004 - reconstruction_loss: 1.8111 - kl_loss: 1.3893\n",
            "Epoch 582/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2014 - reconstruction_loss: 1.8122 - kl_loss: 1.3890\n",
            "Epoch 583/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1985 - reconstruction_loss: 1.8149 - kl_loss: 1.3889\n",
            "Epoch 584/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1986 - reconstruction_loss: 1.8118 - kl_loss: 1.3887\n",
            "Epoch 585/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1975 - reconstruction_loss: 1.8106 - kl_loss: 1.3887\n",
            "Epoch 586/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1971 - reconstruction_loss: 1.8123 - kl_loss: 1.3889\n",
            "Epoch 587/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1971 - reconstruction_loss: 1.8130 - kl_loss: 1.3886\n",
            "Epoch 588/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1977 - reconstruction_loss: 1.8135 - kl_loss: 1.3882\n",
            "Epoch 589/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2006 - reconstruction_loss: 1.8125 - kl_loss: 1.3887\n",
            "Epoch 590/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1996 - reconstruction_loss: 1.8105 - kl_loss: 1.3890\n",
            "Epoch 591/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.2016 - reconstruction_loss: 1.8109 - kl_loss: 1.3891\n",
            "Epoch 592/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1950 - reconstruction_loss: 1.8105 - kl_loss: 1.3886\n",
            "Epoch 593/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1950 - reconstruction_loss: 1.8101 - kl_loss: 1.3887\n",
            "Epoch 594/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1978 - reconstruction_loss: 1.8122 - kl_loss: 1.3885\n",
            "Epoch 595/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1908 - reconstruction_loss: 1.8111 - kl_loss: 1.3884\n",
            "Epoch 596/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1959 - reconstruction_loss: 1.8104 - kl_loss: 1.3884\n",
            "Epoch 597/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1924 - reconstruction_loss: 1.8073 - kl_loss: 1.3884\n",
            "Epoch 598/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1925 - reconstruction_loss: 1.8084 - kl_loss: 1.3886\n",
            "Epoch 599/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1976 - reconstruction_loss: 1.8108 - kl_loss: 1.3888\n",
            "Epoch 600/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1941 - reconstruction_loss: 1.8077 - kl_loss: 1.3887\n",
            "Epoch 601/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1939 - reconstruction_loss: 1.8085 - kl_loss: 1.3882\n",
            "Epoch 602/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1967 - reconstruction_loss: 1.8093 - kl_loss: 1.3883\n",
            "Epoch 603/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1862 - reconstruction_loss: 1.8075 - kl_loss: 1.3880\n",
            "Epoch 604/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1906 - reconstruction_loss: 1.8072 - kl_loss: 1.3884\n",
            "Epoch 605/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1957 - reconstruction_loss: 1.8090 - kl_loss: 1.3885\n",
            "Epoch 606/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1942 - reconstruction_loss: 1.8087 - kl_loss: 1.3878\n",
            "Epoch 607/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1939 - reconstruction_loss: 1.8077 - kl_loss: 1.3880\n",
            "Epoch 608/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1943 - reconstruction_loss: 1.8077 - kl_loss: 1.3882\n",
            "Epoch 609/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1957 - reconstruction_loss: 1.8082 - kl_loss: 1.3885\n",
            "Epoch 610/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1909 - reconstruction_loss: 1.8073 - kl_loss: 1.3880\n",
            "Epoch 611/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1891 - reconstruction_loss: 1.8041 - kl_loss: 1.3880\n",
            "Epoch 612/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1874 - reconstruction_loss: 1.8061 - kl_loss: 1.3878\n",
            "Epoch 613/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1865 - reconstruction_loss: 1.8035 - kl_loss: 1.3879\n",
            "Epoch 614/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1916 - reconstruction_loss: 1.8045 - kl_loss: 1.3877\n",
            "Epoch 615/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1881 - reconstruction_loss: 1.8076 - kl_loss: 1.3879\n",
            "Epoch 616/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1862 - reconstruction_loss: 1.8029 - kl_loss: 1.3881\n",
            "Epoch 617/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1877 - reconstruction_loss: 1.8056 - kl_loss: 1.3882\n",
            "Epoch 618/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1924 - reconstruction_loss: 1.8070 - kl_loss: 1.3880\n",
            "Epoch 619/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1881 - reconstruction_loss: 1.8039 - kl_loss: 1.3874\n",
            "Epoch 620/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1898 - reconstruction_loss: 1.8048 - kl_loss: 1.3876\n",
            "Epoch 621/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1888 - reconstruction_loss: 1.8023 - kl_loss: 1.3877\n",
            "Epoch 622/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1882 - reconstruction_loss: 1.8033 - kl_loss: 1.3878\n",
            "Epoch 623/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1850 - reconstruction_loss: 1.8037 - kl_loss: 1.3876\n",
            "Epoch 624/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1854 - reconstruction_loss: 1.8047 - kl_loss: 1.3875\n",
            "Epoch 625/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1900 - reconstruction_loss: 1.8022 - kl_loss: 1.3879\n",
            "Epoch 626/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1840 - reconstruction_loss: 1.8041 - kl_loss: 1.3874\n",
            "Epoch 627/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1894 - reconstruction_loss: 1.8041 - kl_loss: 1.3876\n",
            "Epoch 628/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1887 - reconstruction_loss: 1.8028 - kl_loss: 1.3877\n",
            "Epoch 629/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1780 - reconstruction_loss: 1.7991 - kl_loss: 1.3874\n",
            "Epoch 630/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1807 - reconstruction_loss: 1.8000 - kl_loss: 1.3875\n",
            "Epoch 631/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1869 - reconstruction_loss: 1.8016 - kl_loss: 1.3870\n",
            "Epoch 632/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1786 - reconstruction_loss: 1.7987 - kl_loss: 1.3872\n",
            "Epoch 633/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1845 - reconstruction_loss: 1.7999 - kl_loss: 1.3874\n",
            "Epoch 634/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1815 - reconstruction_loss: 1.8000 - kl_loss: 1.3874\n",
            "Epoch 635/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1876 - reconstruction_loss: 1.7999 - kl_loss: 1.3875\n",
            "Epoch 636/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1789 - reconstruction_loss: 1.7990 - kl_loss: 1.3872\n",
            "Epoch 637/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1792 - reconstruction_loss: 1.7972 - kl_loss: 1.3873\n",
            "Epoch 638/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1835 - reconstruction_loss: 1.8000 - kl_loss: 1.3875\n",
            "Epoch 639/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1814 - reconstruction_loss: 1.7991 - kl_loss: 1.3871\n",
            "Epoch 640/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1805 - reconstruction_loss: 1.7955 - kl_loss: 1.3870\n",
            "Epoch 641/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1844 - reconstruction_loss: 1.8006 - kl_loss: 1.3873\n",
            "Epoch 642/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1883 - reconstruction_loss: 1.7997 - kl_loss: 1.3872\n",
            "Epoch 643/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1757 - reconstruction_loss: 1.7949 - kl_loss: 1.3869\n",
            "Epoch 644/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1842 - reconstruction_loss: 1.7964 - kl_loss: 1.3870\n",
            "Epoch 645/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1787 - reconstruction_loss: 1.7977 - kl_loss: 1.3870\n",
            "Epoch 646/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1736 - reconstruction_loss: 1.7948 - kl_loss: 1.3865\n",
            "Epoch 647/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1817 - reconstruction_loss: 1.7953 - kl_loss: 1.3871\n",
            "Epoch 648/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1828 - reconstruction_loss: 1.7963 - kl_loss: 1.3871\n",
            "Epoch 649/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1788 - reconstruction_loss: 1.7971 - kl_loss: 1.3867\n",
            "Epoch 650/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1812 - reconstruction_loss: 1.7964 - kl_loss: 1.3870\n",
            "Epoch 651/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1774 - reconstruction_loss: 1.7967 - kl_loss: 1.3867\n",
            "Epoch 652/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1755 - reconstruction_loss: 1.7942 - kl_loss: 1.3869\n",
            "Epoch 653/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1779 - reconstruction_loss: 1.7958 - kl_loss: 1.3870\n",
            "Epoch 654/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1800 - reconstruction_loss: 1.7931 - kl_loss: 1.3867\n",
            "Epoch 655/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1843 - reconstruction_loss: 1.7960 - kl_loss: 1.3869\n",
            "Epoch 656/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1756 - reconstruction_loss: 1.7939 - kl_loss: 1.3867\n",
            "Epoch 657/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1807 - reconstruction_loss: 1.7948 - kl_loss: 1.3866\n",
            "Epoch 658/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1738 - reconstruction_loss: 1.7923 - kl_loss: 1.3866\n",
            "Epoch 659/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1770 - reconstruction_loss: 1.7925 - kl_loss: 1.3866\n",
            "Epoch 660/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1757 - reconstruction_loss: 1.7955 - kl_loss: 1.3865\n",
            "Epoch 661/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1773 - reconstruction_loss: 1.7942 - kl_loss: 1.3863\n",
            "Epoch 662/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1731 - reconstruction_loss: 1.7895 - kl_loss: 1.3861\n",
            "Epoch 663/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1732 - reconstruction_loss: 1.7941 - kl_loss: 1.3866\n",
            "Epoch 664/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1781 - reconstruction_loss: 1.7937 - kl_loss: 1.3866\n",
            "Epoch 665/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1723 - reconstruction_loss: 1.7904 - kl_loss: 1.3862\n",
            "Epoch 666/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1727 - reconstruction_loss: 1.7923 - kl_loss: 1.3864\n",
            "Epoch 667/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1696 - reconstruction_loss: 1.7882 - kl_loss: 1.3861\n",
            "Epoch 668/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1763 - reconstruction_loss: 1.7914 - kl_loss: 1.3863\n",
            "Epoch 669/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1747 - reconstruction_loss: 1.7909 - kl_loss: 1.3861\n",
            "Epoch 670/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1699 - reconstruction_loss: 1.7906 - kl_loss: 1.3862\n",
            "Epoch 671/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1701 - reconstruction_loss: 1.7901 - kl_loss: 1.3864\n",
            "Epoch 672/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1698 - reconstruction_loss: 1.7883 - kl_loss: 1.3866\n",
            "Epoch 673/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1758 - reconstruction_loss: 1.7908 - kl_loss: 1.3863\n",
            "Epoch 674/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1693 - reconstruction_loss: 1.7901 - kl_loss: 1.3860\n",
            "Epoch 675/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1699 - reconstruction_loss: 1.7877 - kl_loss: 1.3862\n",
            "Epoch 676/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1685 - reconstruction_loss: 1.7867 - kl_loss: 1.3863\n",
            "Epoch 677/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1697 - reconstruction_loss: 1.7881 - kl_loss: 1.3860\n",
            "Epoch 678/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1726 - reconstruction_loss: 1.7890 - kl_loss: 1.3865\n",
            "Epoch 679/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1696 - reconstruction_loss: 1.7880 - kl_loss: 1.3861\n",
            "Epoch 680/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1715 - reconstruction_loss: 1.7888 - kl_loss: 1.3860\n",
            "Epoch 681/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1714 - reconstruction_loss: 1.7902 - kl_loss: 1.3861\n",
            "Epoch 682/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1721 - reconstruction_loss: 1.7878 - kl_loss: 1.3863\n",
            "Epoch 683/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1668 - reconstruction_loss: 1.7855 - kl_loss: 1.3858\n",
            "Epoch 684/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1712 - reconstruction_loss: 1.7894 - kl_loss: 1.3856\n",
            "Epoch 685/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1713 - reconstruction_loss: 1.7886 - kl_loss: 1.3859\n",
            "Epoch 686/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1694 - reconstruction_loss: 1.7873 - kl_loss: 1.3861\n",
            "Epoch 687/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1737 - reconstruction_loss: 1.7889 - kl_loss: 1.3859\n",
            "Epoch 688/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1691 - reconstruction_loss: 1.7880 - kl_loss: 1.3855\n",
            "Epoch 689/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1653 - reconstruction_loss: 1.7839 - kl_loss: 1.3858\n",
            "Epoch 690/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1665 - reconstruction_loss: 1.7851 - kl_loss: 1.3861\n",
            "Epoch 691/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1684 - reconstruction_loss: 1.7855 - kl_loss: 1.3856\n",
            "Epoch 692/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1682 - reconstruction_loss: 1.7875 - kl_loss: 1.3857\n",
            "Epoch 693/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1618 - reconstruction_loss: 1.7820 - kl_loss: 1.3858\n",
            "Epoch 694/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1647 - reconstruction_loss: 1.7821 - kl_loss: 1.3856\n",
            "Epoch 695/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1635 - reconstruction_loss: 1.7832 - kl_loss: 1.3858\n",
            "Epoch 696/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1649 - reconstruction_loss: 1.7838 - kl_loss: 1.3859\n",
            "Epoch 697/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1658 - reconstruction_loss: 1.7827 - kl_loss: 1.3854\n",
            "Epoch 698/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1641 - reconstruction_loss: 1.7805 - kl_loss: 1.3861\n",
            "Epoch 699/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1635 - reconstruction_loss: 1.7804 - kl_loss: 1.3854\n",
            "Epoch 700/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1617 - reconstruction_loss: 1.7798 - kl_loss: 1.3859\n",
            "Epoch 701/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1653 - reconstruction_loss: 1.7849 - kl_loss: 1.3854\n",
            "Epoch 702/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1641 - reconstruction_loss: 1.7825 - kl_loss: 1.3854\n",
            "Epoch 703/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1584 - reconstruction_loss: 1.7811 - kl_loss: 1.3853\n",
            "Epoch 704/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1587 - reconstruction_loss: 1.7816 - kl_loss: 1.3853\n",
            "Epoch 705/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1613 - reconstruction_loss: 1.7805 - kl_loss: 1.3852\n",
            "Epoch 706/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1627 - reconstruction_loss: 1.7798 - kl_loss: 1.3853\n",
            "Epoch 707/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1609 - reconstruction_loss: 1.7797 - kl_loss: 1.3852\n",
            "Epoch 708/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1597 - reconstruction_loss: 1.7800 - kl_loss: 1.3854\n",
            "Epoch 709/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1599 - reconstruction_loss: 1.7781 - kl_loss: 1.3853\n",
            "Epoch 710/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1609 - reconstruction_loss: 1.7796 - kl_loss: 1.3853\n",
            "Epoch 711/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1607 - reconstruction_loss: 1.7797 - kl_loss: 1.3853\n",
            "Epoch 712/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1593 - reconstruction_loss: 1.7795 - kl_loss: 1.3853\n",
            "Epoch 713/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1605 - reconstruction_loss: 1.7786 - kl_loss: 1.3852\n",
            "Epoch 714/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1586 - reconstruction_loss: 1.7791 - kl_loss: 1.3850\n",
            "Epoch 715/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1563 - reconstruction_loss: 1.7770 - kl_loss: 1.3855\n",
            "Epoch 716/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1558 - reconstruction_loss: 1.7778 - kl_loss: 1.3853\n",
            "Epoch 717/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1611 - reconstruction_loss: 1.7774 - kl_loss: 1.3850\n",
            "Epoch 718/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1606 - reconstruction_loss: 1.7767 - kl_loss: 1.3849\n",
            "Epoch 719/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1549 - reconstruction_loss: 1.7780 - kl_loss: 1.3850\n",
            "Epoch 720/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1574 - reconstruction_loss: 1.7776 - kl_loss: 1.3851\n",
            "Epoch 721/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1615 - reconstruction_loss: 1.7769 - kl_loss: 1.3851\n",
            "Epoch 722/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1582 - reconstruction_loss: 1.7797 - kl_loss: 1.3847\n",
            "Epoch 723/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1574 - reconstruction_loss: 1.7761 - kl_loss: 1.3852\n",
            "Epoch 724/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1534 - reconstruction_loss: 1.7771 - kl_loss: 1.3848\n",
            "Epoch 725/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1535 - reconstruction_loss: 1.7759 - kl_loss: 1.3849\n",
            "Epoch 726/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1584 - reconstruction_loss: 1.7764 - kl_loss: 1.3850\n",
            "Epoch 727/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1569 - reconstruction_loss: 1.7771 - kl_loss: 1.3849\n",
            "Epoch 728/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1573 - reconstruction_loss: 1.7766 - kl_loss: 1.3847\n",
            "Epoch 729/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1576 - reconstruction_loss: 1.7756 - kl_loss: 1.3850\n",
            "Epoch 730/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1524 - reconstruction_loss: 1.7745 - kl_loss: 1.3843\n",
            "Epoch 731/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1589 - reconstruction_loss: 1.7775 - kl_loss: 1.3850\n",
            "Epoch 732/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1587 - reconstruction_loss: 1.7765 - kl_loss: 1.3846\n",
            "Epoch 733/2000\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 3.1563 - reconstruction_loss: 1.7749 - kl_loss: 1.3846\n",
            "Epoch 734/2000\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 3.1531 - reconstruction_loss: 1.7745 - kl_loss: 1.3847\n",
            "Epoch 735/2000\n",
            "202/391 [==============>...............] - ETA: 0s - loss: 3.1544 - reconstruction_loss: 1.7748 - kl_loss: 1.3851"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-1317761f7b57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtraining_vae_cifar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_maps_cifar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m-> 2955\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3249\u001b[0m       cache_key, cache_key_deletion_observer = function_cache.make_cache_key(\n\u001b[0;32m-> 3250\u001b[0;31m           (args, kwargs))\n\u001b[0m\u001b[1;32m   3251\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3252\u001b[0m       cache_key, cache_key_deletion_observer = function_cache.make_cache_key(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function_cache.py\u001b[0m in \u001b[0;36mmake_cache_key\u001b[0;34m(args, include_tensor_ranks_only)\u001b[0m\n\u001b[1;32m    260\u001b[0m   return FunctionCacheKey(\n\u001b[1;32m    261\u001b[0m       \u001b[0mfunction_signature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m       _make_execution_context()), signature_context.deletion_observer\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function_trace_type.py\u001b[0m in \u001b[0;36mdeletion_observer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdeletion_observer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;34m\"\"\"Returns a functor which invalidates the current key when called.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deletion_observer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=tf.keras.optimizers.Adam(), metrics = ['accuracy'])\n",
        "training_vae_cifar = vae.fit(feat_maps_cifar, epochs=2000, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjOmVGSNPxYT"
      },
      "source": [
        "## Once the training of the VFE is done we freeze it and connect to the feature extractor to train a new classifier that will take encoded representations of feature maps and map it to given classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6kKzT4iPwq8"
      },
      "outputs": [],
      "source": [
        "vae.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yyC7sFhQmvM"
      },
      "outputs": [],
      "source": [
        "def classifier():\n",
        "\n",
        "  inputs = tf.keras.Input(shape = (256,))\n",
        "  x = tf.keras.layers.Dropout(0.3)(inputs)\n",
        "  outputs = tf.keras.layers.Dense(10, activation = 'softmax')(x)\n",
        "  classifier = tf.keras.Model(inputs, outputs, name = 'classifier')\n",
        "  return classifier\n",
        "\n",
        "classifier = classifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lh10n7k1Qv9D"
      },
      "outputs": [],
      "source": [
        "## CREATING THE IMAGE CLASSIFIER WITH VFE\n",
        "\n",
        "model1 = tf.keras.Model(base_model.input, vae.encoder(base_model.output))\n",
        "model_global = tf.keras.Model(model1.input, classifier(model1.output[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAsxwzChQ_TT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3109fefc-23f0-4c05-8b74-dd7f7c5541eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lambda_input (InputLayer)   [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " lambda (Lambda)             (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " densenet121 (Functional)    (None, 7, 7, 1024)        7037504   \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 1024)             0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " encoder (Functional)        [(None, 256),             395008    \n",
            "                              (None, 256),                       \n",
            "                              (None, 256)]                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,432,512\n",
            "Trainable params: 0\n",
            "Non-trainable params: 7,432,512\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYNcFaNuRBSi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fc4d23f-f9de-4d21-c07f-08168cad896b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lambda_input (InputLayer)   [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " lambda (Lambda)             (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " densenet121 (Functional)    (None, 7, 7, 1024)        7037504   \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 1024)             0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " encoder (Functional)        [(None, 256),             395008    \n",
            "                              (None, 256),                       \n",
            "                              (None, 256)]                       \n",
            "                                                                 \n",
            " classifier (Functional)     (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,435,082\n",
            "Trainable params: 2,570\n",
            "Non-trainable params: 7,432,512\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_global.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfeJM6yBRCaS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a65798-9d3b-4786-f873-66a7437403bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "500/500 [==============================] - 139s 266ms/step - loss: 1.2806 - accuracy: 0.6280 - val_loss: 0.5026 - val_accuracy: 0.9200\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 137s 274ms/step - loss: 0.3975 - accuracy: 0.9309 - val_loss: 0.3017 - val_accuracy: 0.9365\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 138s 277ms/step - loss: 0.2781 - accuracy: 0.9452 - val_loss: 0.2415 - val_accuracy: 0.9375\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.2246 - accuracy: 0.9482 - val_loss: 0.2156 - val_accuracy: 0.9410\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 140s 281ms/step - loss: 0.1951 - accuracy: 0.9525 - val_loss: 0.1981 - val_accuracy: 0.9405\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 141s 281ms/step - loss: 0.1787 - accuracy: 0.9529 - val_loss: 0.1883 - val_accuracy: 0.9405\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 141s 281ms/step - loss: 0.1645 - accuracy: 0.9540 - val_loss: 0.1841 - val_accuracy: 0.9390\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 141s 281ms/step - loss: 0.1525 - accuracy: 0.9552 - val_loss: 0.1807 - val_accuracy: 0.9435\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 141s 281ms/step - loss: 0.1479 - accuracy: 0.9553 - val_loss: 0.1766 - val_accuracy: 0.9425\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 140s 281ms/step - loss: 0.1430 - accuracy: 0.9549 - val_loss: 0.1801 - val_accuracy: 0.9440\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 141s 281ms/step - loss: 0.1387 - accuracy: 0.9557 - val_loss: 0.1795 - val_accuracy: 0.9455\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 140s 281ms/step - loss: 0.1321 - accuracy: 0.9567 - val_loss: 0.1778 - val_accuracy: 0.9440\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 140s 281ms/step - loss: 0.1309 - accuracy: 0.9560 - val_loss: 0.1819 - val_accuracy: 0.9420\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 140s 281ms/step - loss: 0.1341 - accuracy: 0.9550 - val_loss: 0.1822 - val_accuracy: 0.9455\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 140s 281ms/step - loss: 0.1319 - accuracy: 0.9551 - val_loss: 0.1809 - val_accuracy: 0.9440\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.1282 - accuracy: 0.9564 - val_loss: 0.1800 - val_accuracy: 0.9440\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 140s 281ms/step - loss: 0.1245 - accuracy: 0.9571 - val_loss: 0.1854 - val_accuracy: 0.9435\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 140s 281ms/step - loss: 0.1258 - accuracy: 0.9574 - val_loss: 0.1837 - val_accuracy: 0.9450\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.1274 - accuracy: 0.9562 - val_loss: 0.1889 - val_accuracy: 0.9435\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.1256 - accuracy: 0.9567 - val_loss: 0.1867 - val_accuracy: 0.9455\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 140s 281ms/step - loss: 0.1271 - accuracy: 0.9557 - val_loss: 0.1837 - val_accuracy: 0.9460\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.1233 - accuracy: 0.9575 - val_loss: 0.1856 - val_accuracy: 0.9460\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.1229 - accuracy: 0.9568 - val_loss: 0.1898 - val_accuracy: 0.9455\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 140s 281ms/step - loss: 0.1237 - accuracy: 0.9567 - val_loss: 0.1900 - val_accuracy: 0.9440\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.1222 - accuracy: 0.9573 - val_loss: 0.1913 - val_accuracy: 0.9425\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 140s 279ms/step - loss: 0.1226 - accuracy: 0.9568 - val_loss: 0.1916 - val_accuracy: 0.9425\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.1235 - accuracy: 0.9558 - val_loss: 0.1907 - val_accuracy: 0.9435\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.1200 - accuracy: 0.9583 - val_loss: 0.1947 - val_accuracy: 0.9425\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 140s 279ms/step - loss: 0.1262 - accuracy: 0.9556 - val_loss: 0.1955 - val_accuracy: 0.9425\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.1241 - accuracy: 0.9562 - val_loss: 0.1929 - val_accuracy: 0.9450\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.1265 - accuracy: 0.9555 - val_loss: 0.1959 - val_accuracy: 0.9445\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.1260 - accuracy: 0.9555 - val_loss: 0.1910 - val_accuracy: 0.9465\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.1242 - accuracy: 0.9563 - val_loss: 0.1925 - val_accuracy: 0.9460\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.1233 - accuracy: 0.9564 - val_loss: 0.1987 - val_accuracy: 0.9440\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 140s 281ms/step - loss: 0.1195 - accuracy: 0.9589 - val_loss: 0.1939 - val_accuracy: 0.9460\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.1234 - accuracy: 0.9566 - val_loss: 0.1929 - val_accuracy: 0.9455\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 140s 279ms/step - loss: 0.1214 - accuracy: 0.9576 - val_loss: 0.1938 - val_accuracy: 0.9450\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.1236 - accuracy: 0.9573 - val_loss: 0.1965 - val_accuracy: 0.9455\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.1218 - accuracy: 0.9572 - val_loss: 0.1996 - val_accuracy: 0.9445\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 139s 278ms/step - loss: 0.1250 - accuracy: 0.9568 - val_loss: 0.2012 - val_accuracy: 0.9445\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 140s 279ms/step - loss: 0.1240 - accuracy: 0.9567 - val_loss: 0.2009 - val_accuracy: 0.9410\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.1253 - accuracy: 0.9559 - val_loss: 0.1994 - val_accuracy: 0.9440\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 140s 279ms/step - loss: 0.1245 - accuracy: 0.9565 - val_loss: 0.2000 - val_accuracy: 0.9440\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.1215 - accuracy: 0.9574 - val_loss: 0.2001 - val_accuracy: 0.9450\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.1208 - accuracy: 0.9576 - val_loss: 0.1990 - val_accuracy: 0.9435\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.1259 - accuracy: 0.9552 - val_loss: 0.2003 - val_accuracy: 0.9430\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.1229 - accuracy: 0.9567 - val_loss: 0.1960 - val_accuracy: 0.9440\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 0.1256 - accuracy: 0.9568 - val_loss: 0.1973 - val_accuracy: 0.9445\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 141s 282ms/step - loss: 0.1201 - accuracy: 0.9575 - val_loss: 0.1992 - val_accuracy: 0.9440\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 141s 281ms/step - loss: 0.1238 - accuracy: 0.9570 - val_loss: 0.1978 - val_accuracy: 0.9455\n"
          ]
        }
      ],
      "source": [
        "model_global.compile(optimizer= 'Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "training = model_global.fit(train_ds, epochs = 50, verbose = 1, validation_data=test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9uTr_bP8__MU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Feature_encoder.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}